{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 10001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.8*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }   \n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "    returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        rew = np.dot(self.model['W2'], h)\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "    ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "    ##    a is an action vector (npar,).                                            ##\n",
    "    ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "    ##    return: the loss per sample (npar,).                                      ##                          \n",
    "    ##                                                                              ##\n",
    "    ## Notes:                                                                       ##\n",
    "    ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "    ##      ai'th output.                                                           ##\n",
    "    ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "    ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "    ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "    ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "    ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "        loss = 0.0\n",
    "        prediction, h = self.forward(s)\n",
    "        diff = np.zeros(npar)\n",
    "        dW2 = np.zeros_like(self.model['W2'])\n",
    "        dW1 = np.zeros_like(self.model['W1'])\n",
    "        num_hidden = h.shape[0]\n",
    "        for i in range(npar):\n",
    "            s_i = s[:, i]\n",
    "            a_i = a[i]\n",
    "            h_i = h[:, i]\n",
    "            pred_i = prediction[a_i, i]\n",
    "            residual = pred_i - y[i]\n",
    "            diff[i] = residual\n",
    "            dW2[a_i, :] += 2*residual*h_i\n",
    "            dh = 2*residual*self.model['W2'][a_i, :]\n",
    "            dh[h_i == 0] = 0\n",
    "            dW1 += np.outer(dh, s_i)\n",
    "        \n",
    "        loss = np.sum(diff ** 2)/npar\n",
    "        dW1 /= npar\n",
    "        dW2 /= npar\n",
    "        \n",
    "        self.grad['W1'] = dW1\n",
    "        self.grad['W2'] = dW2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] -= learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    act_val = estimator.predict(state) # act_val: naction x npar\n",
    "    #print(act_val)\n",
    "    #print(act_val.shape)\n",
    "    nactions, npar = act_val.shape\n",
    "    opt_action = np.argmax(act_val, axis=0)\n",
    "    #print(opt_action)\n",
    "    A = np.zeros((nactions, npar)) \n",
    "    A += epsilon/nactions # the exploration part\n",
    "    for i in range(npar):\n",
    "        #print(i)\n",
    "        A[opt_action[i], i] = 1 - epsilon + (epsilon/nactions)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-23 16:00:09,133] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,198] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,257] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,297] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,328] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 11 timesteps\n",
      "Episode finished after 48 timesteps\n",
      "Episode finished after 79 timesteps\n",
      "Episode finished after 95 timesteps\n",
      "Episode finished after 120 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 172 timesteps\n",
      "Episode finished after 197 timesteps\n",
      "Episode finished after 216 timesteps\n",
      "Episode finished after 234 timesteps\n",
      "Episode finished after 251 timesteps\n",
      "Episode finished after 266 timesteps\n",
      "Episode finished after 280 timesteps\n",
      "Episode finished after 301 timesteps\n",
      "Episode finished after 318 timesteps\n",
      "Episode finished after 327 timesteps\n",
      "Episode finished after 364 timesteps\n",
      "Episode finished after 402 timesteps\n",
      "Episode finished after 459 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 497 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 581 timesteps\n",
      "Episode finished after 594 timesteps\n",
      "Episode finished after 613 timesteps\n",
      "Episode finished after 635 timesteps\n",
      "Episode finished after 655 timesteps\n",
      "Episode finished after 700 timesteps\n",
      "Episode finished after 722 timesteps\n",
      "Episode finished after 758 timesteps\n",
      "Episode finished after 778 timesteps\n",
      "Episode finished after 790 timesteps\n",
      "Episode finished after 815 timesteps\n",
      "Episode finished after 888 timesteps\n",
      "Episode finished after 914 timesteps\n",
      "Episode finished after 970 timesteps\n",
      "Episode finished after 986 timesteps\n",
      "Episode finished after 1012 timesteps\n",
      "Episode finished after 1026 timesteps\n",
      "Episode finished after 1046 timesteps\n",
      "Episode finished after 1066 timesteps\n",
      "Episode finished after 1077 timesteps\n",
      "Episode finished after 1090 timesteps\n",
      "Episode finished after 1127 timesteps\n",
      "Episode finished after 1138 timesteps\n",
      "Episode finished after 1149 timesteps\n",
      "Episode finished after 1160 timesteps\n",
      "Episode finished after 1171 timesteps\n",
      "Episode finished after 1185 timesteps\n",
      "Episode finished after 1206 timesteps\n",
      "Episode finished after 1232 timesteps\n",
      "Episode finished after 1248 timesteps\n",
      "Episode finished after 1266 timesteps\n",
      "Episode finished after 1287 timesteps\n",
      "Episode finished after 1310 timesteps\n",
      "Episode finished after 1354 timesteps\n",
      "Episode finished after 1375 timesteps\n",
      "Episode finished after 1392 timesteps\n",
      "Episode finished after 1410 timesteps\n",
      "Episode finished after 1424 timesteps\n",
      "Episode finished after 1448 timesteps\n",
      "Episode finished after 1475 timesteps\n",
      "Episode finished after 1489 timesteps\n",
      "Episode finished after 1510 timesteps\n",
      "Episode finished after 1526 timesteps\n",
      "Episode finished after 1554 timesteps\n",
      "Episode finished after 1581 timesteps\n",
      "Episode finished after 1595 timesteps\n",
      "Episode finished after 1620 timesteps\n",
      "Episode finished after 1636 timesteps\n",
      "Episode finished after 1651 timesteps\n",
      "Episode finished after 1665 timesteps\n",
      "Episode finished after 1683 timesteps\n",
      "Episode finished after 1694 timesteps\n",
      "Episode finished after 1713 timesteps\n",
      "Episode finished after 1737 timesteps\n",
      "Episode finished after 1782 timesteps\n",
      "Episode finished after 1803 timesteps\n",
      "Episode finished after 1814 timesteps\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 54 timesteps\n",
      "Episode finished after 70 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Episode finished after 111 timesteps\n",
      "Episode finished after 143 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 214 timesteps\n",
      "Episode finished after 235 timesteps\n",
      "Episode finished after 265 timesteps\n",
      "Episode finished after 337 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 363 timesteps\n",
      "Episode finished after 381 timesteps\n",
      "Episode finished after 392 timesteps\n",
      "Episode finished after 411 timesteps\n",
      "Episode finished after 424 timesteps\n",
      "Episode finished after 446 timesteps\n",
      "Episode finished after 460 timesteps\n",
      "Episode finished after 492 timesteps\n",
      "Episode finished after 511 timesteps\n",
      "Episode finished after 525 timesteps\n",
      "Episode finished after 538 timesteps\n",
      "Episode finished after 566 timesteps\n",
      "Episode finished after 578 timesteps\n",
      "Episode finished after 606 timesteps\n",
      "Episode finished after 628 timesteps\n",
      "Episode finished after 647 timesteps\n",
      "Episode finished after 668 timesteps\n",
      "Episode finished after 696 timesteps\n",
      "Episode finished after 713 timesteps\n",
      "Episode finished after 738 timesteps\n",
      "Episode finished after 750 timesteps\n",
      "Episode finished after 769 timesteps\n",
      "Episode finished after 790 timesteps\n",
      "Episode finished after 800 timesteps\n",
      "Episode finished after 810 timesteps\n",
      "Episode finished after 863 timesteps\n",
      "Episode finished after 872 timesteps\n",
      "Episode finished after 910 timesteps\n",
      "Episode finished after 934 timesteps\n",
      "Episode finished after 976 timesteps\n",
      "Episode finished after 995 timesteps\n",
      "Episode finished after 1007 timesteps\n",
      "Episode finished after 1020 timesteps\n",
      "Episode finished after 1048 timesteps\n",
      "Episode finished after 1064 timesteps\n",
      "Episode finished after 1092 timesteps\n",
      "Episode finished after 1112 timesteps\n",
      "Episode finished after 1124 timesteps\n",
      "Episode finished after 1169 timesteps\n",
      "Episode finished after 1193 timesteps\n",
      "Episode finished after 1214 timesteps\n",
      "Episode finished after 1279 timesteps\n",
      "Episode finished after 1306 timesteps\n",
      "Episode finished after 1329 timesteps\n",
      "Episode finished after 1338 timesteps\n",
      "Episode finished after 1356 timesteps\n",
      "Episode finished after 1376 timesteps\n",
      "Episode finished after 1388 timesteps\n",
      "Episode finished after 1402 timesteps\n",
      "Episode finished after 1418 timesteps\n",
      "Episode finished after 1429 timesteps\n",
      "Episode finished after 1449 timesteps\n",
      "Episode finished after 1474 timesteps\n",
      "Episode finished after 1514 timesteps\n",
      "Episode finished after 1527 timesteps\n",
      "Episode finished after 1556 timesteps\n",
      "Episode finished after 1578 timesteps\n",
      "Episode finished after 1624 timesteps\n",
      "Episode finished after 1646 timesteps\n",
      "Episode finished after 1656 timesteps\n",
      "Episode finished after 1668 timesteps\n",
      "Episode finished after 1695 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 38 timesteps\n",
      "Episode finished after 68 timesteps\n",
      "Episode finished after 105 timesteps\n",
      "Episode finished after 115 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 149 timesteps\n",
      "Episode finished after 160 timesteps\n",
      "Episode finished after 180 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 208 timesteps\n",
      "Episode finished after 221 timesteps\n",
      "Episode finished after 239 timesteps\n",
      "Episode finished after 257 timesteps\n",
      "Episode finished after 287 timesteps\n",
      "Episode finished after 300 timesteps\n",
      "Episode finished after 330 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 370 timesteps\n",
      "Episode finished after 379 timesteps\n",
      "Episode finished after 394 timesteps\n",
      "Episode finished after 417 timesteps\n",
      "Episode finished after 437 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 471 timesteps\n",
      "Episode finished after 490 timesteps\n",
      "Episode finished after 509 timesteps\n",
      "Episode finished after 533 timesteps\n",
      "Episode finished after 568 timesteps\n",
      "Episode finished after 580 timesteps\n",
      "Episode finished after 589 timesteps\n",
      "Episode finished after 607 timesteps\n",
      "Episode finished after 621 timesteps\n",
      "Episode finished after 637 timesteps\n",
      "Episode finished after 664 timesteps\n",
      "Episode finished after 697 timesteps\n",
      "Episode finished after 718 timesteps\n",
      "Episode finished after 731 timesteps\n",
      "Episode finished after 741 timesteps\n",
      "Episode finished after 756 timesteps\n",
      "Episode finished after 766 timesteps\n",
      "Episode finished after 781 timesteps\n",
      "Episode finished after 830 timesteps\n",
      "Episode finished after 853 timesteps\n",
      "Episode finished after 864 timesteps\n",
      "Episode finished after 879 timesteps\n",
      "Episode finished after 890 timesteps\n",
      "Episode finished after 907 timesteps\n",
      "Episode finished after 921 timesteps\n",
      "Episode finished after 941 timesteps\n",
      "Episode finished after 955 timesteps\n",
      "Episode finished after 996 timesteps\n",
      "Episode finished after 1040 timesteps\n",
      "Episode finished after 1070 timesteps\n",
      "Episode finished after 1085 timesteps\n",
      "Episode finished after 1099 timesteps\n",
      "Episode finished after 1130 timesteps\n",
      "Episode finished after 1154 timesteps\n",
      "Episode finished after 1166 timesteps\n",
      "Episode finished after 1197 timesteps\n",
      "Episode finished after 1221 timesteps\n",
      "Episode finished after 1250 timesteps\n",
      "Episode finished after 1274 timesteps\n",
      "Episode finished after 1293 timesteps\n",
      "Episode finished after 1310 timesteps\n",
      "Episode finished after 1325 timesteps\n",
      "Episode finished after 1347 timesteps\n",
      "Episode finished after 1359 timesteps\n",
      "Episode finished after 1374 timesteps\n",
      "Episode finished after 1391 timesteps\n",
      "Episode finished after 1415 timesteps\n",
      "Episode finished after 1434 timesteps\n",
      "Episode finished after 1475 timesteps\n",
      "Episode finished after 1486 timesteps\n",
      "Episode finished after 1507 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 64 timesteps\n",
      "Episode finished after 77 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 123 timesteps\n",
      "Episode finished after 148 timesteps\n",
      "Episode finished after 163 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 217 timesteps\n",
      "Episode finished after 229 timesteps\n",
      "Episode finished after 272 timesteps\n",
      "Episode finished after 333 timesteps\n",
      "Episode finished after 344 timesteps\n",
      "Episode finished after 353 timesteps\n",
      "Episode finished after 371 timesteps\n",
      "Episode finished after 400 timesteps\n",
      "Episode finished after 428 timesteps\n",
      "Episode finished after 450 timesteps\n",
      "Episode finished after 476 timesteps\n",
      "Episode finished after 484 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 535 timesteps\n",
      "Episode finished after 550 timesteps\n",
      "Episode finished after 563 timesteps\n",
      "Episode finished after 601 timesteps\n",
      "Episode finished after 629 timesteps\n",
      "Episode finished after 641 timesteps\n",
      "Episode finished after 660 timesteps\n",
      "Episode finished after 676 timesteps\n",
      "Episode finished after 685 timesteps\n",
      "Episode finished after 740 timesteps\n",
      "Episode finished after 752 timesteps\n",
      "Episode finished after 763 timesteps\n",
      "Episode finished after 785 timesteps\n",
      "Episode finished after 798 timesteps\n",
      "Episode finished after 810 timesteps\n",
      "Episode finished after 826 timesteps\n",
      "Episode finished after 838 timesteps\n",
      "Episode finished after 856 timesteps\n",
      "Episode finished after 870 timesteps\n",
      "Episode finished after 894 timesteps\n",
      "Episode finished after 915 timesteps\n",
      "Episode finished after 940 timesteps\n",
      "Episode finished after 958 timesteps\n",
      "Episode finished after 971 timesteps\n",
      "Episode finished after 987 timesteps\n",
      "Episode finished after 1011 timesteps\n",
      "Episode finished after 1045 timesteps\n",
      "Episode finished after 1058 timesteps\n",
      "Episode finished after 1071 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 111 timesteps\n",
      "Episode finished after 135 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 214 timesteps\n",
      "Episode finished after 242 timesteps\n",
      "Episode finished after 259 timesteps\n",
      "Episode finished after 279 timesteps\n",
      "Episode finished after 291 timesteps\n",
      "Episode finished after 303 timesteps\n",
      "Episode finished after 320 timesteps\n",
      "Episode finished after 331 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 371 timesteps\n",
      "Episode finished after 379 timesteps\n",
      "Episode finished after 397 timesteps\n",
      "Episode finished after 415 timesteps\n",
      "Episode finished after 485 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 511 timesteps\n",
      "Episode finished after 531 timesteps\n",
      "Episode finished after 572 timesteps\n",
      "Episode finished after 592 timesteps\n",
      "Episode finished after 603 timesteps\n",
      "Episode finished after 630 timesteps\n",
      "Episode finished after 645 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 707 timesteps\n",
      "Episode finished after 723 timesteps\n",
      "Episode finished after 759 timesteps\n",
      "Episode finished after 798 timesteps\n",
      "Episode finished after 825 timesteps\n",
      "Episode finished after 853 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-23 16:00:09,383] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,440] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,486] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,521] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,552] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 872 timesteps\n",
      "Episode finished after 899 timesteps\n",
      "Episode finished after 914 timesteps\n",
      "Episode finished after 927 timesteps\n",
      "Episode finished after 946 timesteps\n",
      "Episode finished after 971 timesteps\n",
      "Episode finished after 984 timesteps\n",
      "Episode finished after 1007 timesteps\n",
      "Episode finished after 1031 timesteps\n",
      "Episode finished after 1062 timesteps\n",
      "Episode finished after 1080 timesteps\n",
      "Episode finished after 1098 timesteps\n",
      "Episode finished after 1119 timesteps\n",
      "Episode finished after 1133 timesteps\n",
      "Episode finished after 1148 timesteps\n",
      "Episode finished after 1170 timesteps\n",
      "Episode finished after 1202 timesteps\n",
      "Episode finished after 1222 timesteps\n",
      "Episode finished after 1241 timesteps\n",
      "Episode finished after 1255 timesteps\n",
      "Episode finished after 1266 timesteps\n",
      "Episode finished after 1311 timesteps\n",
      "Episode finished after 1346 timesteps\n",
      "Episode finished after 1364 timesteps\n",
      "Episode finished after 1392 timesteps\n",
      "Episode finished after 1402 timesteps\n",
      "Episode finished after 1417 timesteps\n",
      "Episode finished after 1431 timesteps\n",
      "Episode finished after 1441 timesteps\n",
      "Episode finished after 1482 timesteps\n",
      "Episode finished after 1548 timesteps\n",
      "Episode finished after 1562 timesteps\n",
      "Episode finished after 1582 timesteps\n",
      "Episode finished after 1603 timesteps\n",
      "Episode finished after 1619 timesteps\n",
      "Episode finished after 1630 timesteps\n",
      "Episode finished after 1650 timesteps\n",
      "Episode finished after 1660 timesteps\n",
      "Episode finished after 1669 timesteps\n",
      "Episode finished after 1689 timesteps\n",
      "Episode finished after 1705 timesteps\n",
      "Episode finished after 1724 timesteps\n",
      "Episode finished after 1740 timesteps\n",
      "Episode finished after 1755 timesteps\n",
      "Episode finished after 1777 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 51 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 117 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 158 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 218 timesteps\n",
      "Episode finished after 242 timesteps\n",
      "Episode finished after 255 timesteps\n",
      "Episode finished after 278 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 316 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 364 timesteps\n",
      "Episode finished after 395 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 430 timesteps\n",
      "Episode finished after 447 timesteps\n",
      "Episode finished after 471 timesteps\n",
      "Episode finished after 480 timesteps\n",
      "Episode finished after 493 timesteps\n",
      "Episode finished after 504 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 540 timesteps\n",
      "Episode finished after 558 timesteps\n",
      "Episode finished after 571 timesteps\n",
      "Episode finished after 582 timesteps\n",
      "Episode finished after 598 timesteps\n",
      "Episode finished after 616 timesteps\n",
      "Episode finished after 640 timesteps\n",
      "Episode finished after 662 timesteps\n",
      "Episode finished after 679 timesteps\n",
      "Episode finished after 690 timesteps\n",
      "Episode finished after 724 timesteps\n",
      "Episode finished after 830 timesteps\n",
      "Episode finished after 850 timesteps\n",
      "Episode finished after 926 timesteps\n",
      "Episode finished after 947 timesteps\n",
      "Episode finished after 962 timesteps\n",
      "Episode finished after 996 timesteps\n",
      "Episode finished after 1005 timesteps\n",
      "Episode finished after 1016 timesteps\n",
      "Episode finished after 1047 timesteps\n",
      "Episode finished after 1061 timesteps\n",
      "Episode finished after 1074 timesteps\n",
      "Episode finished after 1100 timesteps\n",
      "Episode finished after 1115 timesteps\n",
      "Episode finished after 1128 timesteps\n",
      "Episode finished after 1140 timesteps\n",
      "Episode finished after 1154 timesteps\n",
      "Episode finished after 1174 timesteps\n",
      "Episode finished after 1207 timesteps\n",
      "Episode finished after 1239 timesteps\n",
      "Episode finished after 1257 timesteps\n",
      "Episode finished after 1272 timesteps\n",
      "Episode finished after 1288 timesteps\n",
      "Episode finished after 1304 timesteps\n",
      "Episode finished after 1322 timesteps\n",
      "Episode finished after 1339 timesteps\n",
      "Episode finished after 1381 timesteps\n",
      "Episode finished after 1436 timesteps\n",
      "Episode finished after 1470 timesteps\n",
      "Episode finished after 1511 timesteps\n",
      "Episode finished after 1529 timesteps\n",
      "Episode finished after 1542 timesteps\n",
      "Episode finished after 1555 timesteps\n",
      "Episode finished after 1571 timesteps\n",
      "Episode finished after 1615 timesteps\n",
      "Episode finished after 1640 timesteps\n",
      "Episode finished after 1675 timesteps\n",
      "Episode finished after 1698 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 73 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 103 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 129 timesteps\n",
      "Episode finished after 169 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 216 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 318 timesteps\n",
      "Episode finished after 333 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 366 timesteps\n",
      "Episode finished after 389 timesteps\n",
      "Episode finished after 410 timesteps\n",
      "Episode finished after 427 timesteps\n",
      "Episode finished after 481 timesteps\n",
      "Episode finished after 494 timesteps\n",
      "Episode finished after 525 timesteps\n",
      "Episode finished after 548 timesteps\n",
      "Episode finished after 563 timesteps\n",
      "Episode finished after 578 timesteps\n",
      "Episode finished after 608 timesteps\n",
      "Episode finished after 659 timesteps\n",
      "Episode finished after 672 timesteps\n",
      "Episode finished after 712 timesteps\n",
      "Episode finished after 735 timesteps\n",
      "Episode finished after 755 timesteps\n",
      "Episode finished after 767 timesteps\n",
      "Episode finished after 811 timesteps\n",
      "Episode finished after 851 timesteps\n",
      "Episode finished after 860 timesteps\n",
      "Episode finished after 888 timesteps\n",
      "Episode finished after 904 timesteps\n",
      "Episode finished after 917 timesteps\n",
      "Episode finished after 958 timesteps\n",
      "Episode finished after 995 timesteps\n",
      "Episode finished after 1009 timesteps\n",
      "Episode finished after 1047 timesteps\n",
      "Episode finished after 1060 timesteps\n",
      "Episode finished after 1077 timesteps\n",
      "Episode finished after 1094 timesteps\n",
      "Episode finished after 1134 timesteps\n",
      "Episode finished after 1149 timesteps\n",
      "Episode finished after 1160 timesteps\n",
      "Episode finished after 1172 timesteps\n",
      "Episode finished after 1222 timesteps\n",
      "Episode finished after 1253 timesteps\n",
      "Episode finished after 1266 timesteps\n",
      "Episode finished after 1288 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 77 timesteps\n",
      "Episode finished after 94 timesteps\n",
      "Episode finished after 115 timesteps\n",
      "Episode finished after 128 timesteps\n",
      "Episode finished after 212 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 242 timesteps\n",
      "Episode finished after 303 timesteps\n",
      "Episode finished after 315 timesteps\n",
      "Episode finished after 334 timesteps\n",
      "Episode finished after 360 timesteps\n",
      "Episode finished after 387 timesteps\n",
      "Episode finished after 401 timesteps\n",
      "Episode finished after 417 timesteps\n",
      "Episode finished after 434 timesteps\n",
      "Episode finished after 459 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 516 timesteps\n",
      "Episode finished after 540 timesteps\n",
      "Episode finished after 558 timesteps\n",
      "Episode finished after 687 timesteps\n",
      "Episode finished after 704 timesteps\n",
      "Episode finished after 720 timesteps\n",
      "Episode finished after 739 timesteps\n",
      "Episode finished after 779 timesteps\n",
      "Episode finished after 796 timesteps\n",
      "Episode finished after 812 timesteps\n",
      "Episode finished after 839 timesteps\n",
      "Episode finished after 849 timesteps\n",
      "Episode finished after 859 timesteps\n",
      "Episode finished after 879 timesteps\n",
      "Episode finished after 897 timesteps\n",
      "Episode finished after 917 timesteps\n",
      "Episode finished after 956 timesteps\n",
      "Episode finished after 973 timesteps\n",
      "Episode finished after 984 timesteps\n",
      "Episode finished after 996 timesteps\n",
      "Episode finished after 1007 timesteps\n",
      "Episode finished after 1024 timesteps\n",
      "Episode finished after 1038 timesteps\n",
      "Episode finished after 1057 timesteps\n",
      "Episode finished after 1072 timesteps\n",
      "Episode finished after 1085 timesteps\n",
      "Episode finished after 1100 timesteps\n",
      "Episode finished after 1123 timesteps\n",
      "Episode finished after 1153 timesteps\n",
      "Episode finished after 1200 timesteps\n",
      "Episode finished after 1248 timesteps\n",
      "Episode finished after 1259 timesteps\n",
      "Episode finished after 1282 timesteps\n",
      "Episode finished after 1292 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 57 timesteps\n",
      "Episode finished after 68 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 124 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 177 timesteps\n",
      "Episode finished after 209 timesteps\n",
      "Episode finished after 240 timesteps\n",
      "Episode finished after 297 timesteps\n",
      "Episode finished after 306 timesteps\n",
      "Episode finished after 343 timesteps\n",
      "Episode finished after 383 timesteps\n",
      "Episode finished after 407 timesteps\n",
      "Episode finished after 424 timesteps\n",
      "Episode finished after 438 timesteps\n",
      "Episode finished after 455 timesteps\n",
      "Episode finished after 469 timesteps\n",
      "Episode finished after 482 timesteps\n",
      "Episode finished after 503 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 546 timesteps\n",
      "Episode finished after 572 timesteps\n",
      "Episode finished after 601 timesteps\n",
      "Episode finished after 617 timesteps\n",
      "Episode finished after 645 timesteps\n",
      "Episode finished after 657 timesteps\n",
      "Episode finished after 687 timesteps\n",
      "Episode finished after 709 timesteps\n",
      "Episode finished after 754 timesteps\n",
      "Episode finished after 783 timesteps\n",
      "Episode finished after 805 timesteps\n",
      "Episode finished after 821 timesteps\n",
      "Episode finished after 876 timesteps\n",
      "Episode finished after 888 timesteps\n",
      "Episode finished after 900 timesteps\n",
      "Episode finished after 910 timesteps\n",
      "Episode finished after 927 timesteps\n",
      "Episode finished after 965 timesteps\n",
      "Episode finished after 991 timesteps\n",
      "Episode finished after 1021 timesteps\n",
      "Episode finished after 1037 timesteps\n",
      "Episode finished after 1050 timesteps\n",
      "Episode finished after 1078 timesteps\n",
      "Episode finished after 1100 timesteps\n",
      "Episode finished after 1113 timesteps\n",
      "Episode finished after 1159 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 41 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 85 timesteps\n",
      "Episode finished after 116 timesteps\n",
      "Episode finished after 162 timesteps\n",
      "Episode finished after 220 timesteps\n",
      "Episode finished after 257 timesteps\n",
      "Episode finished after 273 timesteps\n",
      "Episode finished after 282 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-23 16:00:09,609] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,665] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,681] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,705] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,719] Making new env: CartPole-v0\n",
      "[2016-11-23 16:00:09,737] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 304 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 377 timesteps\n",
      "Episode finished after 389 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 434 timesteps\n",
      "Episode finished after 452 timesteps\n",
      "Episode finished after 463 timesteps\n",
      "Episode finished after 510 timesteps\n",
      "Episode finished after 532 timesteps\n",
      "Episode finished after 562 timesteps\n",
      "Episode finished after 573 timesteps\n",
      "Episode finished after 584 timesteps\n",
      "Episode finished after 623 timesteps\n",
      "Episode finished after 643 timesteps\n",
      "Episode finished after 666 timesteps\n",
      "Episode finished after 722 timesteps\n",
      "Episode finished after 744 timesteps\n",
      "Episode finished after 766 timesteps\n",
      "Episode finished after 784 timesteps\n",
      "Episode finished after 793 timesteps\n",
      "Episode finished after 815 timesteps\n",
      "Episode finished after 842 timesteps\n",
      "Episode finished after 860 timesteps\n",
      "Episode finished after 873 timesteps\n",
      "Episode finished after 910 timesteps\n",
      "Episode finished after 933 timesteps\n",
      "Episode finished after 966 timesteps\n",
      "Episode finished after 985 timesteps\n",
      "Episode finished after 1005 timesteps\n",
      "Episode finished after 1015 timesteps\n",
      "Episode finished after 1041 timesteps\n",
      "Episode finished after 1050 timesteps\n",
      "Episode finished after 1098 timesteps\n",
      "Episode finished after 1118 timesteps\n",
      "Episode finished after 1133 timesteps\n",
      "Episode finished after 1152 timesteps\n",
      "Episode finished after 1171 timesteps\n",
      "Episode finished after 1189 timesteps\n",
      "Episode finished after 1214 timesteps\n",
      "Episode finished after 1228 timesteps\n",
      "Episode finished after 1263 timesteps\n",
      "Episode finished after 1286 timesteps\n",
      "Episode finished after 1303 timesteps\n",
      "Episode finished after 1316 timesteps\n",
      "Episode finished after 1329 timesteps\n",
      "Episode finished after 1354 timesteps\n",
      "Episode finished after 1366 timesteps\n",
      "Episode finished after 1379 timesteps\n",
      "Episode finished after 1421 timesteps\n",
      "Episode finished after 1442 timesteps\n",
      "Episode finished after 1466 timesteps\n",
      "Episode finished after 1476 timesteps\n",
      "Episode finished after 1505 timesteps\n",
      "Episode finished after 1517 timesteps\n",
      "Episode finished after 1540 timesteps\n",
      "Episode finished after 1551 timesteps\n",
      "Episode finished after 1579 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 41 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 66 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 129 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 216 timesteps\n",
      "Episode finished after 236 timesteps\n",
      "Episode finished after 261 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 287 timesteps\n",
      "Episode finished after 300 timesteps\n",
      "Episode finished after 320 timesteps\n",
      "Episode finished after 343 timesteps\n",
      "Episode finished after 373 timesteps\n",
      "Episode finished after 393 timesteps\n",
      "Episode finished after 410 timesteps\n",
      "Episode finished after 424 timesteps\n",
      "Episode finished after 466 timesteps\n",
      "Episode finished after 492 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 541 timesteps\n",
      "Episode finished after 565 timesteps\n",
      "Episode finished after 583 timesteps\n",
      "Episode finished after 606 timesteps\n",
      "Episode finished after 637 timesteps\n",
      "Episode finished after 658 timesteps\n",
      "Episode finished after 676 timesteps\n",
      "Episode finished after 698 timesteps\n",
      "Episode finished after 717 timesteps\n",
      "Episode finished after 735 timesteps\n",
      "Episode finished after 745 timesteps\n",
      "Episode finished after 763 timesteps\n",
      "Episode finished after 779 timesteps\n",
      "Episode finished after 803 timesteps\n",
      "Episode finished after 862 timesteps\n",
      "Episode finished after 873 timesteps\n",
      "Episode finished after 901 timesteps\n",
      "Episode finished after 919 timesteps\n",
      "Episode finished after 932 timesteps\n",
      "Episode finished after 945 timesteps\n",
      "Episode finished after 965 timesteps\n",
      "Episode finished after 976 timesteps\n",
      "Episode finished after 996 timesteps\n",
      "Episode finished after 1010 timesteps\n",
      "Episode finished after 1043 timesteps\n",
      "Episode finished after 1064 timesteps\n",
      "Episode finished after 1083 timesteps\n",
      "Episode finished after 1100 timesteps\n",
      "Episode finished after 1113 timesteps\n",
      "Episode finished after 1125 timesteps\n",
      "Episode finished after 1142 timesteps\n",
      "Episode finished after 1152 timesteps\n",
      "Episode finished after 1160 timesteps\n",
      "Episode finished after 1179 timesteps\n",
      "Episode finished after 1194 timesteps\n",
      "Episode finished after 1218 timesteps\n",
      "Episode finished after 1233 timesteps\n",
      "Episode finished after 1246 timesteps\n",
      "Episode finished after 1284 timesteps\n",
      "Episode finished after 1303 timesteps\n",
      "Episode finished after 1322 timesteps\n",
      "Episode finished after 1383 timesteps\n",
      "Episode finished after 1392 timesteps\n",
      "Episode finished after 1407 timesteps\n",
      "Episode finished after 1417 timesteps\n",
      "Episode finished after 1441 timesteps\n",
      "Episode finished after 1451 timesteps\n",
      "Episode finished after 1467 timesteps\n",
      "Episode finished after 1485 timesteps\n",
      "Episode finished after 1505 timesteps\n",
      "Episode finished after 1514 timesteps\n",
      "Episode finished after 1551 timesteps\n",
      "Episode finished after 1566 timesteps\n",
      "Episode finished after 1576 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 101 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 171 timesteps\n",
      "Episode finished after 222 timesteps\n",
      "Episode finished after 252 timesteps\n",
      "Episode finished after 267 timesteps\n",
      "Episode finished after 292 timesteps\n",
      "Episode finished after 311 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 372 timesteps\n",
      "Episode finished after 397 timesteps\n",
      "Episode finished after 412 timesteps\n",
      "Episode finished after 462 timesteps\n",
      "Episode finished after 501 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 82 timesteps\n",
      "Episode finished after 105 timesteps\n",
      "Episode finished after 125 timesteps\n",
      "Episode finished after 138 timesteps\n",
      "Episode finished after 163 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 281 timesteps\n",
      "Episode finished after 305 timesteps\n",
      "Episode finished after 324 timesteps\n",
      "Episode finished after 403 timesteps\n",
      "Episode finished after 420 timesteps\n",
      "Episode finished after 440 timesteps\n",
      "Episode finished after 461 timesteps\n",
      "Episode finished after 490 timesteps\n",
      "Episode finished after 502 timesteps\n",
      "Episode finished after 538 timesteps\n",
      "Episode finished after 567 timesteps\n",
      "Episode finished after 609 timesteps\n",
      "Episode finished after 626 timesteps\n",
      "Episode finished after 652 timesteps\n",
      "Episode finished after 665 timesteps\n",
      "Episode finished after 680 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 709 timesteps\n",
      "Episode finished after 724 timesteps\n",
      "Episode finished after 739 timesteps\n",
      "Episode finished after 763 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 46 timesteps\n",
      "Episode finished after 56 timesteps\n",
      "Episode finished after 68 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Episode finished after 111 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 169 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 220 timesteps\n",
      "Episode finished after 235 timesteps\n",
      "Episode finished after 251 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 295 timesteps\n",
      "Episode finished after 308 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 338 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 64 timesteps\n",
      "Episode finished after 74 timesteps\n",
      "Episode finished after 95 timesteps\n",
      "Episode finished after 116 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 153 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 202 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 237 timesteps\n",
      "Episode finished after 256 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 311 timesteps\n",
      "Episode finished after 331 timesteps\n",
      "Episode finished after 381 timesteps\n",
      "Episode finished after 395 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 75 timesteps\n",
      "Episode finished after 93 timesteps\n",
      "Episode finished after 115 timesteps\n",
      "Episode finished after 153 timesteps\n"
     ]
    }
   ],
   "source": [
    "block_reward = 0.0;\n",
    "total_epochs = 0;\n",
    "   \n",
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n",
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    envs[i] = gym.make(game_type)\n",
    "   \n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    length_game = random.randint(nwindow, init_moves)\n",
    "    observation = envs[i].reset()\n",
    "    for t in range(length_game):\n",
    "        actions[i] = envs[i].action_space.sample()\n",
    "        prev_obs = observation\n",
    "        observation, rewards[i], dones[i], _ = envs[i].step(actions[i])\n",
    "        if dones[i]:\n",
    "            total_epochs += 1\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            rewards[i] = 0\n",
    "            observation = envs[i].reset()\n",
    "        block_reward += rewards[i]\n",
    "    state[:, i] = np.hstack([observation, float(1), prev_obs, float(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.0, loss 0.00030293, epochs 0, reward/epoch 16.00000\n",
      "step 1000, time 1.5, loss 113.77054433, epochs 668, reward/epoch 23.95210\n",
      "step 2000, time 3.0, loss 96.73138246, epochs 1154, reward/epoch 32.92181\n",
      "step 3000, time 4.4, loss 85.56945959, epochs 1501, reward/epoch 46.10951\n",
      "step 4000, time 5.8, loss 69.35525423, epochs 1724, reward/epoch 71.74888\n",
      "step 5000, time 7.2, loss 49.21662579, epochs 1855, reward/epoch 122.13740\n",
      "step 6000, time 8.7, loss 35.32232871, epochs 1941, reward/epoch 186.04651\n",
      "step 7000, time 10.1, loss 30.10062288, epochs 2010, reward/epoch 231.88406\n",
      "step 8000, time 11.6, loss 33.55479037, epochs 2088, reward/epoch 205.12821\n",
      "step 9000, time 13.1, loss 39.53927204, epochs 2174, reward/epoch 186.04651\n",
      "step 10000, time 14.5, loss 33.01584214, epochs 2242, reward/epoch 235.29412\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "block_reward = 0.0\n",
    "last_epochs=0\n",
    "total_epochs=0\n",
    "\n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "  \n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################\n",
    "    next_state = np.zeros_like(state)\n",
    "    indx = int(istep*neps/nsteps)\n",
    "    epsilon = epsilons[indx]\n",
    "    lr_indx = int(istep*neps/nsteps)\n",
    "    lr = learning_rates[lr_indx]\n",
    "    \n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep)\n",
    "    action_probs = policy(q_estimator, state, epsilon)\n",
    "    \n",
    "    for i in range(npar):\n",
    "        current_state = state[:, i]\n",
    "        action_prob = action_probs[:, i]\n",
    "        action = np.random.choice(VALID_ACTIONS, p=action_prob)\n",
    "        observation, rew, done, _ = envs[i].step(action)\n",
    "        rewards[i] = rew\n",
    "        actions[i] = action\n",
    "        dones[i] = done\n",
    "        block_reward += rew\n",
    "        if done:\n",
    "            observation = envs[i].reset()\n",
    "            total_epochs += 1\n",
    "        else: next_state[:, i] = np.concatenate((preprocess(observation), current_state[0:5]))\n",
    "    prediction = target_estimator.predict(next_state)\n",
    "    q_max = np.max(prediction, axis=0)\n",
    "    expected_rewards = rewards\n",
    "    for i in range(npar):\n",
    "        if not dones[i]:\n",
    "            expected_rewards[i] += discount_factor*q_max[i]\n",
    "    loss = q_estimator.gradient(state, actions, expected_rewards)\n",
    "    block_loss += loss\n",
    "    q_estimator.rmsprop(lr, decay_rate=decay_rate)\n",
    "    state = next_state\n",
    "    \n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0): \n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        block_reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
