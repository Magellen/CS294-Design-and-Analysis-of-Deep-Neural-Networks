{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 10001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.8*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }   \n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "    returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s) # nhidden x npars\n",
    "        h[h<0] = 0 # ReLU nonlinearity # nhidden x npars\n",
    "        rew = np.dot(self.model['W2'], h) # nactions, npars\n",
    "        print(type(rew))\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew, h\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "    ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "    ##    a is an action vector (npar,).                                            ##\n",
    "    ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "    ##    return: the loss per sample (npar,).                                      ##                          \n",
    "    ##                                                                              ##\n",
    "    ## Notes:                                                                       ##\n",
    "    ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "    ##      ai'th output.                                                           ##\n",
    "    ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "    ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "    ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "    ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "    ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "        ninputs, npar = s.shape\n",
    "        loss_obs = [] # Collecting L_i\n",
    "        loss = 0.0\n",
    "        # Calculating Loss\n",
    "        # Prediction: nactions x npar, h: nhidden x npar\n",
    "        prediction, h = self.predict(s) \n",
    "        nactions, _ = prediction.shape\n",
    "        predicted_reward = [] \n",
    "        #w2_grads = []\n",
    "        w1_grads = []\n",
    "        # ----- Lambda Function to determine the positivity of deviation ---- #\n",
    "        sign_func = lambda a: (a>0) - (a<0)\n",
    "        self.grad['W1'] = np.zeros_like(self.model['W1'])\n",
    "        self.grad['W2'] = np.zeros_like(self.model['W2'])\n",
    "        \n",
    "        dh = np.zeros_like(h)\n",
    "        for i in range(npar):\n",
    "            sign = sign_func(prediction[a[i],i]-y[i])\n",
    "            loss_i = prediction[a[i], i] - y[i]\n",
    "            loss_i = sign*loss_i\n",
    "            self.grad['W2'][a[i], :] += sign*h[i, :].T\n",
    "            print(dh)\n",
    "            dh[:, i] = sign*(self.model['W2'][a[i],:]).T\n",
    "            loss += loss_i\n",
    "        \n",
    "        self.grad['W1'] = np.dot(dh.T, s)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # For each environment, we are conducting backpropagation\n",
    "            #dscores = np.zeros(nactions)\n",
    "            #dscores[a[i]] = 1\n",
    "            #square_diffs[i] = (prediction[a[i], i] - y[i])**2\n",
    "            #one_hot = np.zeros(nactions)\n",
    "            #one_hot[a[i]] = 2*(prediction[a[i], i] - y[i]) # nactions x 1\n",
    "            #tmp = np.zeros_like(self.model['W2'])\n",
    "            #tmp[a[i],:] = h[:, i]*one_hot[a[i]]\n",
    "            #w2_grads.append(tmp) \n",
    "            \n",
    "            #dhidden = np.dot(dscores, self.model['W2']).T # nhidden x 1\n",
    "            #dhidden[h[:, i] <= 0] = 0 # RELU backprop\n",
    "            \n",
    "            # On W1\n",
    "            #tmp_2 = np.zeros_like(self.model['W1'])\n",
    "            #tmp_2 = np.dot(dhidden, )\n",
    "            # \n",
    "            \n",
    "            \n",
    "        #for i in range(npar):\n",
    "        #    predicted_reward.append(prediction[a[i],i])\n",
    "        #    loss_obs.append(np.sqrt((prediction[a[i], i] - y[i])**2))\n",
    "            \n",
    "        #predicted_reward = np.array(predicted_reward) ### \n",
    "        #loss_obs = np.array(loss_obs)\n",
    "        #tmp_loss = np.linalg.norm(loss_obs)\n",
    "        #loss = tmp_loss/(np.sqrt(npar))\n",
    "        ###### dscroes as the one hot encoding \n",
    "        #dscores = np.zeros_like(prediction)\n",
    "        \n",
    "        # Updating Gradients\n",
    "        #for i in range(npar):\n",
    "        #    dscores[a[i], i] = 1\n",
    "        #    action = a[i]\n",
    "        #    self.grad['W2'][action,:] += h[:,i].T\n",
    "        \n",
    "        #self.grad['W2'] = self.grad['W2']/(2*tmp_loss*np.sqrt(npar))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for i in range(npar):\n",
    "        #    action = a[i]\n",
    "        #    self.grad['W1'] += (1/np.sqrt())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.model['W1'] -= learning_rate*self.grad['W1']\n",
    "        #self.model['W2'] -= learning_rate*self.grad['W2']\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] -= learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    act_val, h = estimator.predict(state) # act_val: naction x npar\n",
    "    #print(act_val)\n",
    "    #print(act_val.shape)\n",
    "    nactions, npar = act_val.shape\n",
    "    opt_action = np.argmax(act_val, axis=0)\n",
    "    #print(opt_action)\n",
    "    A = np.zeros((nactions, npar)) \n",
    "    A += epsilon/nactions # the exploration part\n",
    "    for i in range(npar):\n",
    "        #print(i)\n",
    "        A[opt_action[i], i] = 1 - epsilon + (epsilon/nactions)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.95,\n",
       "         0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05],\n",
       "       [ 0.95,  0.95,  0.95,  0.95,  0.95,  0.95,  0.95,  0.95,  0.05,\n",
       "         0.95,  0.95,  0.95,  0.95,  0.95,  0.95,  0.95]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.random.random((nfeats*nwindow, npar))\n",
    "q_estimator1 = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "policy(q_estimator1, state, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_reward = np.zeros((16), dtype=float);\n",
    "total_epochs = np.zeros((16), dtype=float);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,) (16,)\n"
     ]
    }
   ],
   "source": [
    "print block_reward.shape, total_epochs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfeats * nwindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:14:03,847] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03324417  0.0340781  -0.03300431 -0.01475731]\n",
      "Discrete(2)\n",
      "[ 0.03392573 -0.16055537 -0.03329945  0.26733238]\n",
      "Discrete(2)\n",
      "[ 0.03071462  0.03502561 -0.0279528  -0.03566467]\n",
      "Discrete(2)\n",
      "[ 0.03141514  0.23053703 -0.0286661  -0.33703429]\n",
      "Discrete(2)\n",
      "[ 0.03602588  0.03583449 -0.03540678 -0.05352715]\n",
      "Discrete(2)\n",
      "[ 0.03674257  0.23144577 -0.03647733 -0.35716759]\n",
      "Discrete(2)\n",
      "[ 0.04137148  0.42706682 -0.04362068 -0.66112592]\n",
      "Discrete(2)\n",
      "[ 0.04991282  0.62276774 -0.0568432  -0.96721847]\n",
      "Discrete(2)\n",
      "[ 0.06236817  0.81860504 -0.07618757 -1.27720288]\n",
      "Discrete(2)\n",
      "[ 0.07874027  1.01461119 -0.10173162 -1.5927367 ]\n",
      "Discrete(2)\n",
      "[ 0.0990325   1.21078269 -0.13358636 -1.91533012]\n",
      "Discrete(2)\n",
      "[ 0.12324815  1.40706629 -0.17189296 -2.24629005]\n",
      "Discrete(2)\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.01615801  0.04579409  0.04172871  0.01276206]\n",
      "Discrete(2)\n",
      "[ 0.01707389 -0.14990069  0.04198395  0.31831338]\n",
      "Discrete(2)\n",
      "[ 0.01407588  0.04459893  0.04835022  0.03916069]\n",
      "Discrete(2)\n",
      "[ 0.01496786 -0.15118182  0.04913343  0.34669804]\n",
      "Discrete(2)\n",
      "[ 0.01194422 -0.34696696  0.05606739  0.65446077]\n",
      "Discrete(2)\n",
      "[ 0.00500488 -0.5428229   0.06915661  0.96425821]\n",
      "Discrete(2)\n",
      "[-0.00585157 -0.73880234  0.08844177  1.27784052]\n",
      "Discrete(2)\n",
      "[-0.02062762 -0.9349334   0.11399858  1.59685526]\n",
      "Discrete(2)\n",
      "[-0.03932629 -0.74133254  0.14593569  1.34178195]\n",
      "Discrete(2)\n",
      "[-0.05415294 -0.93795818  0.17277133  1.67633824]\n",
      "Discrete(2)\n",
      "[-0.0729121  -0.74521051  0.20629809  1.44206359]\n",
      "Discrete(2)\n",
      "Episode finished after 11 timesteps\n",
      "[ 0.00734272  0.04068948 -0.02979344 -0.04317549]\n",
      "Discrete(2)\n",
      "[ 0.00815651 -0.15399286 -0.03065695  0.23996044]\n",
      "Discrete(2)\n",
      "[ 0.00507665 -0.34866376 -0.02585774  0.52281784]\n",
      "Discrete(2)\n",
      "[-0.00189662 -0.15318759 -0.01540139  0.22210028]\n",
      "Discrete(2)\n",
      "[-0.00496038  0.04215107 -0.01095938 -0.07540079]\n",
      "Discrete(2)\n",
      "[-0.00411735  0.23742841 -0.0124674  -0.3715212 ]\n",
      "Discrete(2)\n",
      "[  6.31213572e-04   4.32725242e-01  -1.98978214e-02  -6.68109000e-01]\n",
      "Discrete(2)\n",
      "[ 0.00928572  0.23788556 -0.03326    -0.3817569 ]\n",
      "Discrete(2)\n",
      "[ 0.01404343  0.4334636  -0.04089514 -0.68473836]\n",
      "Discrete(2)\n",
      "[ 0.0227127   0.23893258 -0.05458991 -0.40520562]\n",
      "Discrete(2)\n",
      "[ 0.02749135  0.43478448 -0.06269402 -0.71458729]\n",
      "Discrete(2)\n",
      "[ 0.03618704  0.24058386 -0.07698576 -0.44227863]\n",
      "Discrete(2)\n",
      "[ 0.04099872  0.43670598 -0.08583134 -0.75820192]\n",
      "Discrete(2)\n",
      "[ 0.04973284  0.63289927 -0.10099538 -1.07661194]\n",
      "Discrete(2)\n",
      "[ 0.06239082  0.43924593 -0.12252761 -0.81725433]\n",
      "Discrete(2)\n",
      "[ 0.07117574  0.63581308 -0.1388727  -1.14582757]\n",
      "Discrete(2)\n",
      "[ 0.08389201  0.83244815 -0.16178925 -1.47863803]\n",
      "Discrete(2)\n",
      "[ 0.10054097  0.63962871 -0.19136201 -1.24054482]\n",
      "Discrete(2)\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.01727351 -0.02589471  0.03588629 -0.01486435]\n",
      "Discrete(2)\n",
      "[ 0.01675562  0.1686947   0.035589   -0.29601223]\n",
      "Discrete(2)\n",
      "[ 0.02012951 -0.02691605  0.02966875  0.00767906]\n",
      "Discrete(2)\n",
      "[ 0.01959119  0.16776812  0.02982234 -0.27549735]\n",
      "Discrete(2)\n",
      "[ 0.02294655  0.36245217  0.02431239 -0.55862711]\n",
      "Discrete(2)\n",
      "[ 0.0301956   0.55722457  0.01313985 -0.84355221]\n",
      "Discrete(2)\n",
      "[ 0.04134009  0.75216475 -0.0037312  -1.13207425]\n",
      "Discrete(2)\n",
      "[ 0.05638338  0.94733535 -0.02637268 -1.42592508]\n",
      "Discrete(2)\n",
      "[ 0.07533009  0.75254905 -0.05489118 -1.1415999 ]\n",
      "Discrete(2)\n",
      "[ 0.09038107  0.94834376 -0.07772318 -1.45097953]\n",
      "Discrete(2)\n",
      "[ 0.10934795  0.75425812 -0.10674277 -1.18355758]\n",
      "Discrete(2)\n",
      "[ 0.12443311  0.95059065 -0.13041392 -1.50770302]\n",
      "Discrete(2)\n",
      "[ 0.14344492  1.14703031 -0.16056798 -1.83809341]\n",
      "Discrete(2)\n",
      "[ 0.16638553  1.34352117 -0.19732985 -2.1760428 ]\n",
      "Discrete(2)\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.01371092  0.03986144  0.04862424  0.03000541]\n",
      "Discrete(2)\n",
      "[ 0.01450815 -0.15592287  0.04922435  0.33762466]\n",
      "Discrete(2)\n",
      "[ 0.01138969  0.0384653   0.05597685  0.0608617 ]\n",
      "Discrete(2)\n",
      "[ 0.012159   -0.1574127   0.05719408  0.3706671 ]\n",
      "Discrete(2)\n",
      "[ 0.00901074 -0.35329861  0.06460742  0.68082141]\n",
      "Discrete(2)\n",
      "[ 0.00194477 -0.15913067  0.07822385  0.40915852]\n",
      "Discrete(2)\n",
      "[-0.00123784  0.03480012  0.08640702  0.14212624]\n",
      "Discrete(2)\n",
      "[-0.00054184 -0.16144623  0.08924955  0.46076961]\n",
      "Discrete(2)\n",
      "[-0.00377076  0.03230837  0.09846494  0.197499  ]\n",
      "Discrete(2)\n",
      "[-0.0031246  -0.164074    0.10241492  0.51954764]\n",
      "Discrete(2)\n",
      "[-0.00640608  0.02946833  0.11280587  0.26081272]\n",
      "Discrete(2)\n",
      "[-0.00581671 -0.16706804  0.11802213  0.58683818]\n",
      "Discrete(2)\n",
      "[-0.00915807 -0.36362796  0.12975889  0.914245  ]\n",
      "Discrete(2)\n",
      "[-0.01643063 -0.56024364  0.14804379  1.24473128]\n",
      "Discrete(2)\n",
      "[-0.0276355  -0.75692179  0.17293841  1.57988753]\n",
      "Discrete(2)\n",
      "[-0.04277394 -0.95362955  0.20453616  1.92113867]\n",
      "Discrete(2)\n",
      "Episode finished after 16 timesteps\n",
      "[-0.01014207  0.00433287 -0.04708089 -0.04264339]\n",
      "Discrete(2)\n",
      "[-0.01005541  0.20009722 -0.04793376 -0.3498012 ]\n",
      "Discrete(2)\n",
      "[-0.00605347  0.00568859 -0.05492978 -0.07261034]\n",
      "Discrete(2)\n",
      "[-0.0059397  -0.1886046  -0.05638199  0.20224865]\n",
      "Discrete(2)\n",
      "[-0.00971179 -0.38287677 -0.05233701  0.47662632]\n",
      "Discrete(2)\n",
      "[-0.01736933 -0.1870564  -0.04280449  0.16791791]\n",
      "Discrete(2)\n",
      "[-0.02111045  0.00865128 -0.03944613 -0.1379554 ]\n",
      "Discrete(2)\n",
      "[-0.02093743 -0.18588414 -0.04220524  0.14202658]\n",
      "Discrete(2)\n",
      "[-0.02465511  0.00981606 -0.03936471 -0.16366696]\n",
      "Discrete(2)\n",
      "[-0.02445879 -0.18472091 -0.04263805  0.1163422 ]\n",
      "Discrete(2)\n",
      "[-0.02815321 -0.37920682 -0.0403112   0.39527414]\n",
      "Discrete(2)\n",
      "[-0.03573734 -0.18353677 -0.03240572  0.09015921]\n",
      "Discrete(2)\n",
      "[-0.03940808 -0.37817961 -0.03060253  0.37244457]\n",
      "Discrete(2)\n",
      "[-0.04697167 -0.18263658 -0.02315364  0.07027163]\n",
      "Discrete(2)\n",
      "[-0.0506244   0.01280953 -0.02174821 -0.22962553]\n",
      "Discrete(2)\n",
      "[-0.05036821  0.2082354  -0.02634072 -0.52908848]\n",
      "Discrete(2)\n",
      "[-0.0462035   0.40371783 -0.03692249 -0.82995368]\n",
      "Discrete(2)\n",
      "[-0.03812915  0.59932451 -0.05352156 -1.13401648]\n",
      "Discrete(2)\n",
      "[-0.02614266  0.79510446 -0.07620189 -1.44299371]\n",
      "Discrete(2)\n",
      "[-0.01024057  0.60099879 -0.10506177 -1.17506182]\n",
      "Discrete(2)\n",
      "[ 0.00177941  0.79731718 -0.128563   -1.49874704]\n",
      "Discrete(2)\n",
      "[ 0.01772575  0.99374544 -0.15853795 -1.82865129]\n",
      "Discrete(2)\n",
      "[ 0.03760066  0.80069627 -0.19511097 -1.58912418]\n",
      "Discrete(2)\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.04616592  0.03417079  0.01399173 -0.00392492]\n",
      "Discrete(2)\n",
      "[ 0.04684934  0.22908932  0.01391323 -0.29216065]\n",
      "Discrete(2)\n",
      "[ 0.05143112  0.03377178  0.00807001  0.00487766]\n",
      "Discrete(2)\n",
      "[ 0.05210656 -0.16146497  0.00816757  0.3000958 ]\n",
      "Discrete(2)\n",
      "[ 0.04887726  0.03353962  0.01416948  0.00999992]\n",
      "Discrete(2)\n",
      "[ 0.04954805  0.22845552  0.01436948 -0.2781789 ]\n",
      "Discrete(2)\n",
      "[ 0.05411716  0.03313156  0.0088059   0.01900128]\n",
      "Discrete(2)\n",
      "[ 0.0547798   0.22812612  0.00918593 -0.27089032]\n",
      "Discrete(2)\n",
      "[ 0.05934232  0.0328743   0.00376812  0.02467568]\n",
      "Discrete(2)\n",
      "[ 0.0599998  -0.16230148  0.00426164  0.3185451 ]\n",
      "Discrete(2)\n",
      "[ 0.05675377  0.03275951  0.01063254  0.02720918]\n",
      "Discrete(2)\n",
      "[ 0.05740896 -0.16251329  0.01117672  0.32322774]\n",
      "Discrete(2)\n",
      "[ 0.0541587  -0.3577926   0.01764128  0.61941429]\n",
      "Discrete(2)\n",
      "[ 0.04700285 -0.55315644  0.03002956  0.91760073]\n",
      "Discrete(2)\n",
      "[ 0.03593972 -0.35845308  0.04838158  0.63450472]\n",
      "Discrete(2)\n",
      "[ 0.02877066 -0.16403819  0.06107167  0.35744226]\n",
      "Discrete(2)\n",
      "[ 0.02548989 -0.35997281  0.06822052  0.66874018]\n",
      "Discrete(2)\n",
      "[ 0.01829044 -0.16586246  0.08159532  0.39829323]\n",
      "Discrete(2)\n",
      "[ 0.01497319 -0.36204145  0.08956118  0.71554508]\n",
      "Discrete(2)\n",
      "[ 0.00773236 -0.55828144  0.10387209  1.03502108]\n",
      "Discrete(2)\n",
      "[-0.00343327 -0.75461956  0.12457251  1.35842443]\n",
      "Discrete(2)\n",
      "[-0.01852566 -0.95106404  0.151741    1.6873374 ]\n",
      "Discrete(2)\n",
      "[-0.03754694 -1.1475805   0.18548774  2.02316581]\n",
      "Discrete(2)\n",
      "Episode finished after 23 timesteps\n",
      "[-0.02303598 -0.01141243 -0.00973265  0.0370381 ]\n",
      "Discrete(2)\n",
      "[-0.02326422 -0.20639348 -0.00899189  0.32663446]\n",
      "Discrete(2)\n",
      "[-0.02739209 -0.01114467 -0.0024592   0.0311295 ]\n",
      "Discrete(2)\n",
      "[-0.02761499 -0.20623126 -0.00183661  0.32303551]\n",
      "Discrete(2)\n",
      "[-0.03173961 -0.01108321  0.0046241   0.02977396]\n",
      "Discrete(2)\n",
      "[-0.03196128  0.18397213  0.00521958 -0.26144643]\n",
      "Discrete(2)\n",
      "[ -2.82818336e-02   3.79019181e-01  -9.34863161e-06  -5.52478482e-01]\n",
      "Discrete(2)\n",
      "[-0.02070145  0.57414126 -0.01105892 -0.84516435]\n",
      "Discrete(2)\n",
      "[-0.00921862  0.76941235 -0.02796221 -1.14130433]\n",
      "Discrete(2)\n",
      "[ 0.00616962  0.57466682 -0.05078829 -0.85752004]\n",
      "Discrete(2)\n",
      "[ 0.01766296  0.77044256 -0.06793869 -1.16573032]\n",
      "Discrete(2)\n",
      "[ 0.03307181  0.96637978 -0.0912533  -1.47891674]\n",
      "Discrete(2)\n",
      "[ 0.05239941  1.16248957 -0.12083163 -1.79864876]\n",
      "Discrete(2)\n",
      "[ 0.0756492   1.35873852 -0.15680461 -2.12631395]\n",
      "Discrete(2)\n",
      "[ 0.10282397  1.16548472 -0.19933089 -1.88590156]\n",
      "Discrete(2)\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.03197589  0.02680599 -0.03643913  0.04285649]\n",
      "Discrete(2)\n",
      "[ 0.03251201  0.22243101 -0.035582   -0.26109697]\n",
      "Discrete(2)\n",
      "[ 0.03696063  0.02783458 -0.04080394  0.02015401]\n",
      "Discrete(2)\n",
      "[ 0.03751732 -0.16667917 -0.04040086  0.29968873]\n",
      "Discrete(2)\n",
      "[ 0.03418373  0.02899467 -0.03440709 -0.00545713]\n",
      "Discrete(2)\n",
      "[ 0.03476363 -0.16561738 -0.03451623  0.27617447]\n",
      "Discrete(2)\n",
      "[ 0.03145128 -0.36023032 -0.02899274  0.55777425]\n",
      "Discrete(2)\n",
      "[ 0.02424667 -0.55493353 -0.01783726  0.84118365]\n",
      "Discrete(2)\n",
      "[  1.31480032e-02  -7.49807499e-01  -1.01358382e-03   1.12820423e+00]\n",
      "Discrete(2)\n",
      "[-0.00184815 -0.55467228  0.0215505   0.83520357]\n",
      "Discrete(2)\n",
      "[-0.01294159 -0.35985126  0.03825457  0.54937524]\n",
      "Discrete(2)\n",
      "[-0.02013862 -0.5554891   0.04924208  0.85386141]\n",
      "Discrete(2)\n",
      "[-0.0312484  -0.7512464   0.06631931  1.16161275]\n",
      "Discrete(2)\n",
      "[-0.04627333 -0.55704794  0.08955156  0.89043902]\n",
      "Discrete(2)\n",
      "[-0.05741429 -0.75326333  0.10736034  1.20987571]\n",
      "Discrete(2)\n",
      "[-0.07247955 -0.55967889  0.13155785  0.95267426]\n",
      "Discrete(2)\n",
      "[-0.08367313 -0.366549    0.15061134  0.70404928]\n",
      "Discrete(2)\n",
      "[-0.09100411 -0.17379947  0.16469233  0.46231053]\n",
      "Discrete(2)\n",
      "[-0.0944801   0.0186587   0.17393854  0.22573051]\n",
      "Discrete(2)\n",
      "[-0.09410693 -0.17846717  0.17845315  0.56783806]\n",
      "Discrete(2)\n",
      "[-0.09767627 -0.37558411  0.18980991  0.91100519]\n",
      "Discrete(2)\n",
      "[-0.10518795 -0.57269712  0.20803001  1.25683414]\n",
      "Discrete(2)\n",
      "Episode finished after 22 timesteps\n",
      "[-0.00997036  0.00687447 -0.0337975  -0.01748418]\n",
      "Discrete(2)\n",
      "[-0.00983287 -0.18774689 -0.03414718  0.26434658]\n",
      "Discrete(2)\n",
      "[-0.01358781  0.00784539 -0.02886025 -0.03890813]\n",
      "Discrete(2)\n",
      "[-0.0134309   0.20336905 -0.02963842 -0.34055523]\n",
      "Discrete(2)\n",
      "[-0.00936352  0.3988999  -0.03644952 -0.64243511]\n",
      "Discrete(2)\n",
      "[-0.00138553  0.20430446 -0.04929822 -0.36144961]\n",
      "Discrete(2)\n",
      "[ 0.00270056  0.4000912  -0.05652721 -0.66926074]\n",
      "Discrete(2)\n",
      "[ 0.01070239  0.2057989  -0.06991243 -0.39489793]\n",
      "Discrete(2)\n",
      "[ 0.01481837  0.01173499 -0.07781039 -0.1250507 ]\n",
      "Discrete(2)\n",
      "[ 0.01505307  0.20788046 -0.0803114  -0.44123186]\n",
      "Discrete(2)\n",
      "[ 0.01921067  0.01398142 -0.08913604 -0.17490695]\n",
      "Discrete(2)\n",
      "[ 0.0194903   0.21025849 -0.09263418 -0.49432503]\n",
      "Discrete(2)\n",
      "[ 0.02369547  0.40655641 -0.10252068 -0.814705  ]\n",
      "Discrete(2)\n",
      "[ 0.0318266   0.21297656 -0.11881478 -0.55594815]\n",
      "Discrete(2)\n",
      "[ 0.03608613  0.01970555 -0.12993374 -0.30293495]\n",
      "Discrete(2)\n",
      "[ 0.03648024  0.2164168  -0.13599244 -0.63360768]\n",
      "Discrete(2)\n",
      "[ 0.04080858  0.02342767 -0.14866459 -0.38665536]\n",
      "Discrete(2)\n",
      "[ 0.04127713  0.22031276 -0.1563977  -0.72227203]\n",
      "Discrete(2)\n",
      "[ 0.04568339  0.02766011 -0.17084314 -0.48261381]\n",
      "Discrete(2)\n",
      "[ 0.04623659  0.22472937 -0.18049542 -0.82389814]\n",
      "Discrete(2)\n",
      "[ 0.05073118  0.03247459 -0.19697338 -0.59297876]\n",
      "Discrete(2)\n",
      "[ 0.05138067  0.22972901 -0.20883296 -0.94067546]\n",
      "Discrete(2)\n",
      "Episode finished after 22 timesteps\n",
      "[-0.0321801   0.0013002   0.03616516  0.03767224]\n",
      "Discrete(2)\n",
      "[-0.03215409  0.19588538  0.0369186  -0.24338443]\n",
      "Discrete(2)\n",
      "[-0.02823639  0.00025608  0.03205091  0.06071117]\n",
      "Discrete(2)\n",
      "[-0.02823126 -0.1953104   0.03326514  0.36333159]\n",
      "Discrete(2)\n",
      "[-0.03213747 -0.39088896  0.04053177  0.66631527]\n",
      "Discrete(2)\n",
      "[-0.03995525 -0.19635348  0.05385807  0.38666479]\n",
      "Discrete(2)\n",
      "[-0.04388232 -0.39219696  0.06159137  0.69583067]\n",
      "Discrete(2)\n",
      "[-0.05172626 -0.19798085  0.07550798  0.42315532]\n",
      "Discrete(2)\n",
      "[-0.05568588 -0.39408675  0.08397109  0.73865426]\n",
      "Discrete(2)\n",
      "[-0.06356761 -0.20021856  0.09874418  0.47353511]\n",
      "Discrete(2)\n",
      "[-0.06757198 -0.39658625  0.10821488  0.79563545]\n",
      "Discrete(2)\n",
      "[-0.07550371 -0.59301372  0.12412759  1.12030625]\n",
      "Discrete(2)\n",
      "[-0.08736398 -0.7895256   0.14653371  1.44920602]\n",
      "Discrete(2)\n",
      "[-0.10315449 -0.98611327  0.17551783  1.78385423]\n",
      "Discrete(2)\n",
      "Episode finished after 14 timesteps\n",
      "[-0.01672001 -0.01978685  0.00049945  0.0053282 ]\n",
      "Discrete(2)\n",
      "[-0.01711575  0.17532793  0.00060601 -0.2871971 ]\n",
      "Discrete(2)\n",
      "[-0.01360919 -0.01980265 -0.00513793  0.0056769 ]\n",
      "Discrete(2)\n",
      "[-0.01400524 -0.21485054 -0.00502439  0.29673433]\n",
      "Discrete(2)\n",
      "[-0.01830225 -0.01965733  0.0009103   0.00247103]\n",
      "Discrete(2)\n",
      "[-0.0186954  -0.21479232  0.00095972  0.29544103]\n",
      "Discrete(2)\n",
      "[-0.02299125 -0.40992794  0.00686854  0.58842648]\n",
      "Discrete(2)\n",
      "[-0.03118981 -0.6051454   0.01863707  0.8832651 ]\n",
      "Discrete(2)\n",
      "[-0.04329271 -0.41028144  0.03630237  0.5964989 ]\n",
      "Discrete(2)\n",
      "[-0.05149834 -0.60589211  0.04823235  0.90039214]\n",
      "Discrete(2)\n",
      "[-0.06361619 -0.80163328  0.06624019  1.20783725]\n",
      "Discrete(2)\n",
      "[-0.07964885 -0.60742659  0.09039694  0.93662645]\n",
      "Discrete(2)\n",
      "[-0.09179738 -0.80364365  0.10912946  1.25629082]\n",
      "Discrete(2)\n",
      "[-0.10787026 -0.61007468  0.13425528  0.99968502]\n",
      "Discrete(2)\n",
      "[-0.12007175 -0.80671071  0.15424898  1.33133744]\n",
      "Discrete(2)\n",
      "[-0.13620596 -1.0034045   0.18087573  1.66804475]\n",
      "Discrete(2)\n",
      "Episode finished after 16 timesteps\n",
      "[-0.04668893  0.01008165 -0.01638558 -0.03130758]\n",
      "Discrete(2)\n",
      "[-0.04648729  0.2054347  -0.01701173 -0.32911496]\n",
      "Discrete(2)\n",
      "[-0.0423786   0.010559   -0.02359403 -0.04184492]\n",
      "Discrete(2)\n",
      "[-0.04216742 -0.18421681 -0.02443093  0.24330148]\n",
      "Discrete(2)\n",
      "[-0.04585176 -0.37898143 -0.0195649   0.52817925]\n",
      "Discrete(2)\n",
      "[-0.05343138 -0.18358976 -0.00900131  0.22939612]\n",
      "Discrete(2)\n",
      "[-0.05710318  0.01165965 -0.00441339 -0.06611249]\n",
      "Discrete(2)\n",
      "[-0.05686999 -0.18339874 -0.00573564  0.22517473]\n",
      "Discrete(2)\n",
      "[-0.06053796 -0.37843825 -0.00123215  0.51604291]\n",
      "Discrete(2)\n",
      "[-0.06810673 -0.57354283  0.00908871  0.8083373 ]\n",
      "Discrete(2)\n",
      "[-0.07957758 -0.76878815  0.02525546  1.10386523]\n",
      "Discrete(2)\n",
      "[-0.09495335 -0.96423301  0.04733276  1.40436334]\n",
      "Discrete(2)\n",
      "[-0.11423801 -0.76972968  0.07542003  1.12684575]\n",
      "Discrete(2)\n",
      "[-0.1296326  -0.9657544   0.09795694  1.44219942]\n",
      "Discrete(2)\n",
      "[-0.14894769 -0.77196549  0.12680093  1.18166289]\n",
      "Discrete(2)\n",
      "[-0.164387   -0.96848429  0.15043419  1.51125412]\n",
      "Discrete(2)\n",
      "[-0.18375668 -1.16507427  0.18065927  1.84686971]\n",
      "Discrete(2)\n",
      "Episode finished after 17 timesteps\n",
      "[-0.0117448  -0.02720716  0.03640015 -0.00352606]\n",
      "Discrete(2)\n",
      "[-0.01228894  0.16737437  0.03632963 -0.28450573]\n",
      "Discrete(2)\n",
      "[-0.00894146  0.36195984  0.03063952 -0.56551278]\n",
      "Discrete(2)\n",
      "[-0.00170226  0.55663884  0.01932926 -0.84838762]\n",
      "Discrete(2)\n",
      "[ 0.00943052  0.36125866  0.00236151 -0.54968964]\n",
      "Discrete(2)\n",
      "[ 0.01665569  0.16610362 -0.00863228 -0.25626361]\n",
      "Discrete(2)\n",
      "[ 0.01997776  0.36134775 -0.01375756 -0.55165675]\n",
      "Discrete(2)\n",
      "[ 0.02720472  0.55666019 -0.02479069 -0.84864228]\n",
      "Discrete(2)\n",
      "[ 0.03833792  0.75211132 -0.04176354 -1.14901662]\n",
      "Discrete(2)\n",
      "[ 0.05338015  0.9477528  -0.06474387 -1.45449785]\n",
      "Discrete(2)\n",
      "[ 0.0723352   0.75348274 -0.09383383 -1.18272471]\n",
      "Discrete(2)\n",
      "[ 0.08740486  0.55969523 -0.11748832 -0.92086888]\n",
      "Discrete(2)\n",
      "[ 0.09859876  0.36634019 -0.1359057  -0.6672979 ]\n",
      "Discrete(2)\n",
      "[ 0.10592557  0.56306422 -0.14925166 -0.99949635]\n",
      "Discrete(2)\n",
      "[ 0.11718685  0.75983174 -0.16924158 -1.33508358]\n",
      "Discrete(2)\n",
      "[ 0.13238349  0.56719786 -0.19594325 -1.09978089]\n",
      "Discrete(2)\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.00378348  0.03038893  0.04268583  0.03625577]\n",
      "Discrete(2)\n",
      "[ 0.00439126 -0.16531832  0.04341094  0.34209507]\n",
      "Discrete(2)\n",
      "[ 0.0010849  -0.36103014  0.05025284  0.64814504]\n",
      "Discrete(2)\n",
      "[-0.00613571 -0.16664298  0.06321575  0.37170051]\n",
      "Discrete(2)\n",
      "[-0.00946857 -0.36260338  0.07064976  0.68362704]\n",
      "Discrete(2)\n",
      "[-0.01672063 -0.16852986  0.0843223   0.41399673]\n",
      "Discrete(2)\n",
      "[-0.02009123  0.02530199  0.09260223  0.14904337]\n",
      "Discrete(2)\n",
      "[-0.01958519  0.21898429  0.0955831  -0.11304916]\n",
      "Discrete(2)\n",
      "[-0.01520551  0.41261591  0.09332211 -0.37411217]\n",
      "Discrete(2)\n",
      "[-0.00695319  0.21630081  0.08583987 -0.05352397]\n",
      "Discrete(2)\n",
      "[-0.00262717  0.02005961  0.08476939  0.26495994]\n",
      "Discrete(2)\n",
      "[-0.00222598 -0.17616352  0.09006859  0.5831301 ]\n",
      "Discrete(2)\n",
      "[-0.00574925  0.01758883  0.10173119  0.32012399]\n",
      "Discrete(2)\n",
      "[-0.00539747  0.21112596  0.10813367  0.06117664]\n",
      "Discrete(2)\n",
      "[-0.00117495  0.40454473  0.10935721 -0.19552756]\n",
      "Discrete(2)\n",
      "[ 0.00691594  0.20804224  0.10544665  0.12955241]\n",
      "Discrete(2)\n",
      "[ 0.01107678  0.40150802  0.1080377  -0.1280905 ]\n",
      "Discrete(2)\n",
      "[ 0.01910695  0.59492976  0.10547589 -0.38483018]\n",
      "Discrete(2)\n",
      "[ 0.03100554  0.78840852  0.09777929 -0.64248301]\n",
      "Discrete(2)\n",
      "[ 0.04677371  0.98204138  0.08492963 -0.90284362]\n",
      "Discrete(2)\n",
      "[ 0.06641454  0.78587801  0.06687276 -0.58471982]\n",
      "Discrete(2)\n",
      "[ 0.0821321   0.58988618  0.05517836 -0.27174325]\n",
      "Discrete(2)\n",
      "[ 0.09392982  0.78417914  0.04974349 -0.54652492]\n",
      "Discrete(2)\n",
      "[ 0.10961341  0.97856819  0.038813   -0.82312927]\n",
      "Discrete(2)\n",
      "[ 0.12918477  0.78293737  0.02235041 -0.51849588]\n",
      "Discrete(2)\n",
      "[ 0.14484352  0.58750799  0.01198049 -0.21885455]\n",
      "Discrete(2)\n",
      "[ 0.15659368  0.39221685  0.0076034   0.07758332]\n",
      "Discrete(2)\n",
      "[ 0.16443801  0.58722898  0.00915507 -0.21269104]\n",
      "Discrete(2)\n",
      "[ 0.17618259  0.78221885  0.00490125 -0.50247204]\n",
      "Discrete(2)\n",
      "[ 0.19182697  0.58702816 -0.00514819 -0.20824856]\n",
      "Discrete(2)\n",
      "[ 0.20356753  0.78222334 -0.00931316 -0.50255102]\n",
      "Discrete(2)\n",
      "[ 0.219212    0.97747532 -0.01936418 -0.79815431]\n",
      "Discrete(2)\n",
      "[ 0.23876151  1.17285752 -0.03532727 -1.09686538]\n",
      "Discrete(2)\n",
      "[ 0.26221866  1.36842638 -0.05726458 -1.4004197 ]\n",
      "Discrete(2)\n",
      "[ 0.28958718  1.56421136 -0.08527297 -1.71044237]\n",
      "Discrete(2)\n",
      "[ 0.32087141  1.37016633 -0.11948182 -1.4454723 ]\n",
      "Discrete(2)\n",
      "[ 0.34827474  1.17669981 -0.14839127 -1.19238563]\n",
      "Discrete(2)\n",
      "[ 0.37180873  0.98377839 -0.17223898 -0.94965085]\n",
      "Discrete(2)\n",
      "[ 0.3914843   1.18074779 -0.19123199 -1.29112152]\n",
      "Discrete(2)\n",
      "Episode finished after 39 timesteps\n",
      "[ 0.04741434 -0.00650544  0.01223924  0.04087407]\n",
      "Discrete(2)\n",
      "[ 0.04728423  0.18843888  0.01305673 -0.24792226]\n",
      "Discrete(2)\n",
      "[ 0.05105301 -0.00686708  0.00809828  0.04885028]\n",
      "Discrete(2)\n",
      "[ 0.05091566  0.18813782  0.00907529 -0.24126661]\n",
      "Discrete(2)\n",
      "[ 0.05467842  0.38312896  0.00424995 -0.53107319]\n",
      "Discrete(2)\n",
      "[ 0.062341    0.18794749 -0.00637151 -0.23705414]\n",
      "Discrete(2)\n",
      "[ 0.06609995 -0.00708286 -0.01111259  0.05361223]\n",
      "Discrete(2)\n",
      "[ 0.06595829 -0.20204372 -0.01004035  0.34276844]\n",
      "Discrete(2)\n",
      "[ 0.06191742 -0.00678038 -0.00318498  0.04693637]\n",
      "Discrete(2)\n",
      "[ 0.06178181 -0.20185651 -0.00224625  0.33861271]\n",
      "Discrete(2)\n",
      "[ 0.05774468 -0.39694643  0.004526    0.63058645]\n",
      "Discrete(2)\n",
      "[ 0.04980575 -0.20188792  0.01713773  0.33933233]\n",
      "Discrete(2)\n",
      "[ 0.04576799 -0.00701396  0.02392438  0.05210256]\n",
      "Discrete(2)\n",
      "[ 0.04562771  0.18775691  0.02496643 -0.23293705]\n",
      "Discrete(2)\n",
      "[ 0.04938285  0.3825134   0.02030769 -0.51764137]\n",
      "Discrete(2)\n",
      "[ 0.05703312  0.18711149  0.00995486 -0.21862889]\n",
      "Discrete(2)\n",
      "[ 0.06077535  0.38208973  0.00558228 -0.50815509]\n",
      "Discrete(2)\n",
      "[ 0.06841715  0.18688958 -0.00458082 -0.21371823]\n",
      "Discrete(2)\n",
      "[ 0.07215494 -0.00816658 -0.00885518  0.07751618]\n",
      "Discrete(2)\n",
      "[ 0.07199161 -0.20316048 -0.00730486  0.36739217]\n",
      "Discrete(2)\n",
      "[  6.79283956e-02  -3.98177870e-01   4.29826011e-05   6.57762841e-01]\n",
      "Discrete(2)\n",
      "[ 0.05996484 -0.20305652  0.01319824  0.36509345]\n",
      "Discrete(2)\n",
      "[ 0.05590371 -0.00812459  0.02050011  0.0766012 ]\n",
      "Discrete(2)\n",
      "[ 0.05574122  0.18669756  0.02203213 -0.20954402]\n",
      "Discrete(2)\n",
      "[ 0.05947517 -0.00873237  0.01784125  0.09000666]\n",
      "Discrete(2)\n",
      "[ 0.05930052  0.18612937  0.01964139 -0.19699438]\n",
      "Discrete(2)\n",
      "[ 0.06302311 -0.00926794  0.0157015   0.10181924]\n",
      "Discrete(2)\n",
      "[ 0.06283775 -0.20461136  0.01773788  0.39941429]\n",
      "Discrete(2)\n",
      "[ 0.05874552 -0.00974546  0.02572617  0.1123761 ]\n",
      "Discrete(2)\n",
      "[ 0.05855061  0.18499859  0.02797369 -0.17208065]\n",
      "Discrete(2)\n",
      "[ 0.06225058  0.37970923  0.02453208 -0.45580915]\n",
      "Discrete(2)\n",
      "[ 0.06984477  0.57447589  0.01541589 -0.74065953]\n",
      "Discrete(2)\n",
      "[  8.13342860e-02   7.69381652e-01   6.02703406e-04  -1.02845134e+00]\n",
      "Discrete(2)\n",
      "[ 0.09672192  0.96449558 -0.01996632 -1.32094498]\n",
      "Discrete(2)\n",
      "[ 0.11601183  0.76963159 -0.04638522 -1.03457697]\n",
      "Discrete(2)\n",
      "[ 0.13140446  0.57515609 -0.06707676 -0.75680986]\n",
      "Discrete(2)\n",
      "[ 0.14290758  0.77113526 -0.08221296 -1.06982332]\n",
      "Discrete(2)\n",
      "[ 0.15833029  0.9672425  -0.10360943 -1.38713401]\n",
      "Discrete(2)\n",
      "[ 0.17767514  1.163492   -0.13135211 -1.71033634]\n",
      "Discrete(2)\n",
      "[ 0.20094498  1.35985588 -0.16555883 -2.04085142]\n",
      "Discrete(2)\n",
      "[ 0.2281421   1.55624906 -0.20637586 -2.37986533]\n",
      "Discrete(2)\n",
      "Episode finished after 41 timesteps\n",
      "[ 0.01524831 -0.03618386 -0.01682097 -0.00622618]\n",
      "Discrete(2)\n",
      "[ 0.01452463  0.15917523 -0.0169455  -0.3041685 ]\n",
      "Discrete(2)\n",
      "[ 0.01770814  0.35453453 -0.02302887 -0.60214711]\n",
      "Discrete(2)\n",
      "[ 0.02479883  0.5499709  -0.03507181 -0.90199383]\n",
      "Discrete(2)\n",
      "[ 0.03579824  0.74555    -0.05311169 -1.20549106]\n",
      "Discrete(2)\n",
      "[ 0.05070924  0.55115313 -0.07722151 -0.92991444]\n",
      "Discrete(2)\n",
      "[ 0.06173231  0.35715365 -0.09581979 -0.66246298]\n",
      "Discrete(2)\n",
      "[ 0.06887538  0.16348625 -0.10906905 -0.4014224 ]\n",
      "Discrete(2)\n",
      "[ 0.07214511 -0.02993329 -0.1170975  -0.14501984]\n",
      "Discrete(2)\n",
      "[ 0.07154644  0.16665397 -0.1199979  -0.47222941]\n",
      "Discrete(2)\n",
      "[ 0.07487952  0.3632482  -0.12944249 -0.80019494]\n",
      "Discrete(2)\n",
      "[ 0.08214448  0.55988551 -0.14544639 -1.13063321]\n",
      "Discrete(2)\n",
      "[ 0.09334219  0.36693599 -0.16805905 -0.8868755 ]\n",
      "Discrete(2)\n",
      "[ 0.10068091  0.56389146 -0.18579656 -1.22732353]\n",
      "Discrete(2)\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.02214099 -0.01295556 -0.02854996  0.00162664]\n",
      "Discrete(2)\n",
      "[ 0.02188188  0.18256396 -0.02851743 -0.29992567]\n",
      "Discrete(2)\n",
      "[ 0.02553316  0.37808054 -0.03451594 -0.60146428]\n",
      "Discrete(2)\n",
      "[ 0.03309477  0.57366788 -0.04654522 -0.90481623]\n",
      "Discrete(2)\n",
      "[ 0.04456813  0.76938823 -0.06464155 -1.21175814]\n",
      "Discrete(2)\n",
      "[ 0.0599559   0.96528231 -0.08887671 -1.52397695]\n",
      "Discrete(2)\n",
      "[ 0.07926154  0.77133896 -0.11935625 -1.26030551]\n",
      "Discrete(2)\n",
      "[ 0.09468832  0.57792868 -0.14456236 -1.00726159]\n",
      "Discrete(2)\n",
      "[ 0.10624689  0.38500157 -0.16470759 -0.763243  ]\n",
      "Discrete(2)\n",
      "[ 0.11394693  0.19248479 -0.17997245 -0.52658141]\n",
      "Discrete(2)\n",
      "[ 0.11779662  0.38962182 -0.19050408 -0.87013763]\n",
      "Discrete(2)\n",
      "[ 0.12558906  0.5867532  -0.20790683 -1.21615526]\n",
      "Discrete(2)\n",
      "Episode finished after 12 timesteps\n",
      "[-0.02407061 -0.03627904 -0.01635722  0.03691191]\n",
      "Discrete(2)\n",
      "[-0.02479619  0.15907361 -0.01561898 -0.26088667]\n",
      "Discrete(2)\n",
      "[-0.02161472 -0.03582194 -0.02083671  0.0268292 ]\n",
      "Discrete(2)\n",
      "[-0.02233116 -0.23063897 -0.02030013  0.31286576]\n",
      "Discrete(2)\n",
      "[-0.02694394 -0.0352338  -0.01404281  0.01385057]\n",
      "Discrete(2)\n",
      "[-0.02764862 -0.23015157 -0.0137658   0.30206995]\n",
      "Discrete(2)\n",
      "[-0.03225165 -0.03483616 -0.0077244   0.00507756]\n",
      "Discrete(2)\n",
      "[-0.03294837 -0.22984648 -0.00762285  0.29531337]\n",
      "Discrete(2)\n",
      "[-0.0375453  -0.03461669 -0.00171658  0.0002361 ]\n",
      "Discrete(2)\n",
      "[-0.03823763 -0.22971398 -0.00171186  0.29237693]\n",
      "Discrete(2)\n",
      "[-0.04283191 -0.42481148  0.00413568  0.58451946]\n",
      "Discrete(2)\n",
      "[-0.05132814 -0.61999112  0.01582606  0.8785023 ]\n",
      "Discrete(2)\n",
      "[-0.06372797 -0.81532451  0.03339611  1.17611836]\n",
      "Discrete(2)\n",
      "[-0.08003446 -0.62065197  0.05691848  0.89408901]\n",
      "Discrete(2)\n",
      "[-0.09244749 -0.4263462   0.07480026  0.61982734]\n",
      "Discrete(2)\n",
      "[-0.10097442 -0.23234426  0.08719681  0.35160892]\n",
      "Discrete(2)\n",
      "[-0.1056213  -0.428591    0.09422898  0.67046405]\n",
      "Discrete(2)\n",
      "[-0.11419312 -0.23489665  0.10763826  0.40887378]\n",
      "Discrete(2)\n",
      "[-0.11889106 -0.43136696  0.11581574  0.73345824]\n",
      "Discrete(2)\n",
      "[-0.1275184  -0.23801943  0.13048491  0.4793536 ]\n",
      "Discrete(2)\n",
      "[-0.13227879 -0.43471872  0.14007198  0.8101481 ]\n",
      "Discrete(2)\n",
      "[-0.14097316 -0.63145335  0.15627494  1.14340642]\n",
      "Discrete(2)\n",
      "[-0.15360223 -0.82823289  0.17914307  1.48073683]\n",
      "Discrete(2)\n",
      "[-0.17016688 -1.02503104  0.2087578   1.82359674]\n",
      "Discrete(2)\n",
      "Episode finished after 24 timesteps\n",
      "[-0.00153099 -0.04828267 -0.03485513 -0.03811646]\n",
      "Discrete(2)\n",
      "[-0.00249664  0.14732132 -0.03561746 -0.34158957]\n",
      "Discrete(2)\n",
      "[  4.49785391e-04   3.42931444e-01  -4.24492490e-02  -6.45287980e-01]\n",
      "Discrete(2)\n",
      "[ 0.00730841  0.14842594 -0.05535501 -0.36626888]\n",
      "Discrete(2)\n",
      "[ 0.01027693  0.34428903 -0.06268039 -0.67587957]\n",
      "Discrete(2)\n",
      "[ 0.01716271  0.15009149 -0.07619798 -0.40357127]\n",
      "Discrete(2)\n",
      "[ 0.02016454 -0.04387175 -0.0842694  -0.13585117]\n",
      "Discrete(2)\n",
      "[ 0.01928711  0.1523498  -0.08698643 -0.45388494]\n",
      "Discrete(2)\n",
      "[ 0.0223341   0.34858715 -0.09606413 -0.77266978]\n",
      "Discrete(2)\n",
      "[ 0.02930585  0.54489033 -0.11151752 -1.09396637]\n",
      "Discrete(2)\n",
      "[ 0.04020365  0.35139988 -0.13339685 -0.83825177]\n",
      "Discrete(2)\n",
      "[ 0.04723165  0.54806659 -0.15016188 -1.16973347]\n",
      "Discrete(2)\n",
      "[ 0.05819298  0.74478802 -0.17355655 -1.50547688]\n",
      "Discrete(2)\n",
      "[ 0.07308874  0.55214395 -0.20366609 -1.27162181]\n",
      "Discrete(2)\n",
      "Episode finished after 14 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        print(env.action_space)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done: \n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-401d431f2beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "envs[i].reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_prev = np.zeros((4))\n",
    "observation_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:14:30,270] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 0)\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 32 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 100 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 177 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 210 timesteps\n",
      "Episode finished after 222 timesteps\n",
      "Episode finished after 277 timesteps\n",
      "Episode finished after 294 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 366 timesteps\n",
      "Episode finished after 423 timesteps\n",
      "Episode finished after 441 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 470 timesteps\n",
      "Episode finished after 489 timesteps\n",
      "Episode finished after 498 timesteps\n",
      "Episode finished after 530 timesteps\n",
      "Episode finished after 546 timesteps\n",
      "Episode finished after 582 timesteps\n",
      "Episode finished after 601 timesteps\n",
      "Episode finished after 616 timesteps\n",
      "Episode finished after 630 timesteps\n",
      "Episode finished after 661 timesteps\n",
      "Episode finished after 675 timesteps\n",
      "Episode finished after 690 timesteps\n",
      "Episode finished after 705 timesteps\n",
      "Episode finished after 716 timesteps\n",
      "Episode finished after 730 timesteps\n",
      "Episode finished after 757 timesteps\n",
      "Episode finished after 818 timesteps\n",
      "Episode finished after 840 timesteps\n",
      "Episode finished after 881 timesteps\n",
      "Episode finished after 919 timesteps\n",
      "Episode finished after 940 timesteps\n",
      "Episode finished after 973 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:14:47,225] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1005 timesteps\n",
      "('agent', 1)\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 54 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 83 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 113 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 166 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 205 timesteps\n",
      "Episode finished after 233 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 266 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 359 timesteps\n",
      "Episode finished after 379 timesteps\n",
      "Episode finished after 394 timesteps\n",
      "Episode finished after 417 timesteps\n",
      "Episode finished after 445 timesteps\n",
      "Episode finished after 456 timesteps\n",
      "Episode finished after 464 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 507 timesteps\n",
      "Episode finished after 554 timesteps\n",
      "Episode finished after 569 timesteps\n",
      "Episode finished after 604 timesteps\n",
      "Episode finished after 625 timesteps\n",
      "Episode finished after 680 timesteps\n",
      "Episode finished after 696 timesteps\n",
      "Episode finished after 733 timesteps\n",
      "Episode finished after 757 timesteps\n",
      "Episode finished after 773 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:00,814] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 807 timesteps\n",
      "('agent', 2)\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 52 timesteps\n",
      "Episode finished after 79 timesteps\n",
      "Episode finished after 99 timesteps\n",
      "Episode finished after 117 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 148 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 210 timesteps\n",
      "Episode finished after 239 timesteps\n",
      "Episode finished after 263 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 305 timesteps\n",
      "Episode finished after 316 timesteps\n",
      "Episode finished after 350 timesteps\n",
      "Episode finished after 366 timesteps\n",
      "Episode finished after 386 timesteps\n",
      "Episode finished after 398 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:07,869] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 3)\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 58 timesteps\n",
      "Episode finished after 73 timesteps\n",
      "Episode finished after 93 timesteps\n",
      "Episode finished after 112 timesteps\n",
      "Episode finished after 137 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 232 timesteps\n",
      "Episode finished after 248 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 283 timesteps\n",
      "Episode finished after 315 timesteps\n",
      "Episode finished after 330 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 362 timesteps\n",
      "Episode finished after 384 timesteps\n",
      "Episode finished after 432 timesteps\n",
      "Episode finished after 446 timesteps\n",
      "Episode finished after 461 timesteps\n",
      "Episode finished after 497 timesteps\n",
      "Episode finished after 511 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:16,739] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 4)\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 52 timesteps\n",
      "Episode finished after 106 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 147 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 218 timesteps\n",
      "Episode finished after 230 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 256 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 292 timesteps\n",
      "Episode finished after 306 timesteps\n",
      "Episode finished after 337 timesteps\n",
      "Episode finished after 372 timesteps\n",
      "Episode finished after 391 timesteps\n",
      "Episode finished after 412 timesteps\n",
      "Episode finished after 438 timesteps\n",
      "Episode finished after 452 timesteps\n",
      "Episode finished after 487 timesteps\n",
      "Episode finished after 539 timesteps\n",
      "Episode finished after 552 timesteps\n",
      "Episode finished after 570 timesteps\n",
      "Episode finished after 579 timesteps\n",
      "Episode finished after 643 timesteps\n",
      "Episode finished after 661 timesteps\n",
      "Episode finished after 677 timesteps\n",
      "Episode finished after 700 timesteps\n",
      "Episode finished after 713 timesteps\n",
      "Episode finished after 728 timesteps\n",
      "Episode finished after 752 timesteps\n",
      "Episode finished after 764 timesteps\n",
      "Episode finished after 774 timesteps\n",
      "Episode finished after 808 timesteps\n",
      "Episode finished after 824 timesteps\n",
      "Episode finished after 867 timesteps\n",
      "Episode finished after 912 timesteps\n",
      "Episode finished after 931 timesteps\n",
      "Episode finished after 945 timesteps\n",
      "Episode finished after 967 timesteps\n",
      "Episode finished after 988 timesteps\n",
      "Episode finished after 1000 timesteps\n",
      "Episode finished after 1049 timesteps\n",
      "Episode finished after 1060 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:37,416] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 5)\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 73 timesteps\n",
      "Episode finished after 85 timesteps\n",
      "Episode finished after 107 timesteps\n",
      "Episode finished after 122 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 205 timesteps\n",
      "Episode finished after 228 timesteps\n",
      "Episode finished after 267 timesteps\n",
      "Episode finished after 288 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 324 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 361 timesteps\n",
      "Episode finished after 370 timesteps\n",
      "Episode finished after 434 timesteps\n",
      "Episode finished after 444 timesteps\n",
      "Episode finished after 458 timesteps\n",
      "Episode finished after 477 timesteps\n",
      "Episode finished after 502 timesteps\n",
      "Episode finished after 513 timesteps\n",
      "Episode finished after 532 timesteps\n",
      "Episode finished after 569 timesteps\n",
      "Episode finished after 599 timesteps\n",
      "Episode finished after 612 timesteps\n",
      "Episode finished after 626 timesteps\n",
      "Episode finished after 640 timesteps\n",
      "Episode finished after 657 timesteps\n",
      "Episode finished after 707 timesteps\n",
      "Episode finished after 720 timesteps\n",
      "Episode finished after 743 timesteps\n",
      "Episode finished after 761 timesteps\n",
      "Episode finished after 776 timesteps\n",
      "Episode finished after 793 timesteps\n",
      "Episode finished after 805 timesteps\n",
      "Episode finished after 830 timesteps\n",
      "Episode finished after 853 timesteps\n",
      "Episode finished after 876 timesteps\n",
      "Episode finished after 892 timesteps\n",
      "Episode finished after 942 timesteps\n",
      "Episode finished after 981 timesteps\n",
      "Episode finished after 1004 timesteps\n",
      "Episode finished after 1030 timesteps\n",
      "Episode finished after 1059 timesteps\n",
      "Episode finished after 1089 timesteps\n",
      "Episode finished after 1102 timesteps\n",
      "Episode finished after 1127 timesteps\n",
      "Episode finished after 1138 timesteps\n",
      "Episode finished after 1157 timesteps\n",
      "Episode finished after 1179 timesteps\n",
      "Episode finished after 1194 timesteps\n",
      "Episode finished after 1211 timesteps\n",
      "Episode finished after 1220 timesteps\n",
      "Episode finished after 1245 timesteps\n",
      "Episode finished after 1264 timesteps\n",
      "Episode finished after 1285 timesteps\n",
      "Episode finished after 1300 timesteps\n",
      "Episode finished after 1317 timesteps\n",
      "Episode finished after 1346 timesteps\n",
      "Episode finished after 1370 timesteps\n",
      "Episode finished after 1388 timesteps\n",
      "Episode finished after 1399 timesteps\n",
      "Episode finished after 1415 timesteps\n",
      "Episode finished after 1453 timesteps\n",
      "Episode finished after 1471 timesteps\n",
      "Episode finished after 1486 timesteps\n",
      "Episode finished after 1498 timesteps\n",
      "Episode finished after 1509 timesteps\n",
      "Episode finished after 1531 timesteps\n",
      "Episode finished after 1545 timesteps\n",
      "Episode finished after 1559 timesteps\n",
      "Episode finished after 1572 timesteps\n",
      "Episode finished after 1613 timesteps\n",
      "Episode finished after 1626 timesteps\n",
      "Episode finished after 1659 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:52,502] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1717 timesteps\n",
      "Episode finished after 1739 timesteps\n",
      "Episode finished after 1766 timesteps\n",
      "Episode finished after 1800 timesteps\n",
      "Episode finished after 1835 timesteps\n",
      "Episode finished after 1872 timesteps\n",
      "('agent', 6)\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 50 timesteps\n",
      "Episode finished after 63 timesteps\n",
      "Episode finished after 74 timesteps\n",
      "Episode finished after 112 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 252 timesteps\n",
      "Episode finished after 273 timesteps\n",
      "Episode finished after 299 timesteps\n",
      "Episode finished after 311 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 335 timesteps\n",
      "Episode finished after 353 timesteps\n",
      "Episode finished after 364 timesteps\n",
      "Episode finished after 375 timesteps\n",
      "Episode finished after 391 timesteps\n",
      "Episode finished after 400 timesteps\n",
      "Episode finished after 413 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:54,178] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 423 timesteps\n",
      "Episode finished after 441 timesteps\n",
      "Episode finished after 476 timesteps\n",
      "Episode finished after 492 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 543 timesteps\n",
      "('agent', 7)\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 59 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 145 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 186 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 239 timesteps\n",
      "Episode finished after 294 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 446 timesteps\n",
      "Episode finished after 461 timesteps\n",
      "Episode finished after 488 timesteps\n",
      "Episode finished after 510 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 538 timesteps\n",
      "Episode finished after 551 timesteps\n",
      "Episode finished after 567 timesteps\n",
      "Episode finished after 587 timesteps\n",
      "Episode finished after 605 timesteps\n",
      "Episode finished after 641 timesteps\n",
      "Episode finished after 653 timesteps\n",
      "Episode finished after 678 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 703 timesteps\n",
      "Episode finished after 719 timesteps\n",
      "Episode finished after 756 timesteps\n",
      "Episode finished after 776 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:15:54,980] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 794 timesteps\n",
      "Episode finished after 807 timesteps\n",
      "Episode finished after 832 timesteps\n",
      "('agent', 8)\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 48 timesteps\n",
      "Episode finished after 67 timesteps\n",
      "Episode finished after 81 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 124 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 228 timesteps\n",
      "Episode finished after 252 timesteps\n",
      "Episode finished after 285 timesteps\n",
      "Episode finished after 298 timesteps\n",
      "Episode finished after 314 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 365 timesteps\n",
      "Episode finished after 382 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 414 timesteps\n",
      "Episode finished after 434 timesteps\n",
      "Episode finished after 457 timesteps\n",
      "Episode finished after 475 timesteps\n",
      "Episode finished after 487 timesteps\n",
      "Episode finished after 497 timesteps\n",
      "Episode finished after 510 timesteps\n",
      "Episode finished after 541 timesteps\n",
      "Episode finished after 566 timesteps\n",
      "Episode finished after 586 timesteps\n",
      "Episode finished after 616 timesteps\n",
      "Episode finished after 630 timesteps\n",
      "Episode finished after 642 timesteps\n",
      "Episode finished after 681 timesteps\n",
      "Episode finished after 722 timesteps\n",
      "Episode finished after 746 timesteps\n",
      "Episode finished after 763 timesteps\n",
      "Episode finished after 774 timesteps\n",
      "Episode finished after 787 timesteps\n",
      "Episode finished after 796 timesteps\n",
      "Episode finished after 811 timesteps\n",
      "Episode finished after 837 timesteps\n",
      "Episode finished after 854 timesteps\n",
      "Episode finished after 892 timesteps\n",
      "Episode finished after 903 timesteps\n",
      "Episode finished after 916 timesteps\n",
      "Episode finished after 934 timesteps\n",
      "Episode finished after 963 timesteps\n",
      "Episode finished after 987 timesteps\n",
      "Episode finished after 1011 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:16:17,903] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 9)\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 60 timesteps\n",
      "Episode finished after 72 timesteps\n",
      "Episode finished after 99 timesteps\n",
      "Episode finished after 114 timesteps\n",
      "Episode finished after 133 timesteps\n",
      "Episode finished after 156 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:16:22,271] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 182 timesteps\n",
      "('agent', 10)\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 57 timesteps\n",
      "Episode finished after 68 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 117 timesteps\n",
      "Episode finished after 143 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 237 timesteps\n",
      "Episode finished after 257 timesteps\n",
      "Episode finished after 289 timesteps\n",
      "Episode finished after 306 timesteps\n",
      "Episode finished after 318 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 353 timesteps\n",
      "Episode finished after 402 timesteps\n",
      "Episode finished after 415 timesteps\n",
      "Episode finished after 435 timesteps\n",
      "Episode finished after 459 timesteps\n",
      "Episode finished after 472 timesteps\n",
      "Episode finished after 505 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 554 timesteps\n",
      "Episode finished after 574 timesteps\n",
      "Episode finished after 599 timesteps\n",
      "Episode finished after 611 timesteps\n",
      "Episode finished after 628 timesteps\n",
      "Episode finished after 647 timesteps\n",
      "Episode finished after 663 timesteps\n",
      "Episode finished after 714 timesteps\n",
      "Episode finished after 739 timesteps\n",
      "Episode finished after 749 timesteps\n",
      "Episode finished after 784 timesteps\n",
      "Episode finished after 810 timesteps\n",
      "Episode finished after 833 timesteps\n",
      "Episode finished after 847 timesteps\n",
      "Episode finished after 865 timesteps\n",
      "Episode finished after 903 timesteps\n",
      "Episode finished after 921 timesteps\n",
      "Episode finished after 929 timesteps\n",
      "Episode finished after 939 timesteps\n",
      "Episode finished after 955 timesteps\n",
      "Episode finished after 1002 timesteps\n",
      "Episode finished after 1016 timesteps\n",
      "Episode finished after 1029 timesteps\n",
      "Episode finished after 1040 timesteps\n",
      "Episode finished after 1093 timesteps\n",
      "Episode finished after 1107 timesteps\n",
      "Episode finished after 1132 timesteps\n",
      "Episode finished after 1163 timesteps\n",
      "Episode finished after 1192 timesteps\n",
      "Episode finished after 1209 timesteps\n",
      "Episode finished after 1233 timesteps\n",
      "Episode finished after 1252 timesteps\n",
      "Episode finished after 1264 timesteps\n",
      "Episode finished after 1285 timesteps\n",
      "Episode finished after 1296 timesteps\n",
      "Episode finished after 1314 timesteps\n",
      "Episode finished after 1360 timesteps\n",
      "Episode finished after 1377 timesteps\n",
      "Episode finished after 1394 timesteps\n",
      "Episode finished after 1422 timesteps\n",
      "Episode finished after 1444 timesteps\n",
      "Episode finished after 1458 timesteps\n",
      "Episode finished after 1478 timesteps\n",
      "Episode finished after 1509 timesteps\n",
      "Episode finished after 1530 timesteps\n",
      "Episode finished after 1557 timesteps\n",
      "Episode finished after 1572 timesteps\n",
      "Episode finished after 1599 timesteps\n",
      "Episode finished after 1623 timesteps\n",
      "Episode finished after 1641 timesteps\n",
      "Episode finished after 1652 timesteps\n",
      "Episode finished after 1670 timesteps\n",
      "Episode finished after 1683 timesteps\n",
      "Episode finished after 1694 timesteps\n",
      "Episode finished after 1720 timesteps\n",
      "Episode finished after 1735 timesteps\n",
      "Episode finished after 1773 timesteps\n",
      "Episode finished after 1807 timesteps\n",
      "Episode finished after 1826 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:16:32,076] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1855 timesteps\n",
      "('agent', 11)\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 29 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 67 timesteps\n",
      "Episode finished after 101 timesteps\n",
      "Episode finished after 159 timesteps\n",
      "Episode finished after 181 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 214 timesteps\n",
      "Episode finished after 242 timesteps\n",
      "Episode finished after 264 timesteps\n",
      "Episode finished after 277 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:16:37,963] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 315 timesteps\n",
      "('agent', 12)\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 129 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 163 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 234 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 318 timesteps\n",
      "Episode finished after 339 timesteps\n",
      "Episode finished after 366 timesteps\n",
      "Episode finished after 379 timesteps\n",
      "Episode finished after 397 timesteps\n",
      "Episode finished after 430 timesteps\n",
      "Episode finished after 458 timesteps\n",
      "Episode finished after 470 timesteps\n",
      "Episode finished after 488 timesteps\n",
      "Episode finished after 521 timesteps\n",
      "Episode finished after 531 timesteps\n",
      "Episode finished after 551 timesteps\n",
      "Episode finished after 572 timesteps\n",
      "Episode finished after 592 timesteps\n",
      "Episode finished after 625 timesteps\n",
      "Episode finished after 636 timesteps\n",
      "Episode finished after 671 timesteps\n",
      "Episode finished after 709 timesteps\n",
      "Episode finished after 720 timesteps\n",
      "Episode finished after 743 timesteps\n",
      "Episode finished after 752 timesteps\n",
      "Episode finished after 807 timesteps\n",
      "Episode finished after 826 timesteps\n",
      "Episode finished after 895 timesteps\n",
      "Episode finished after 907 timesteps\n",
      "Episode finished after 922 timesteps\n",
      "Episode finished after 938 timesteps\n",
      "Episode finished after 964 timesteps\n",
      "Episode finished after 1002 timesteps\n",
      "Episode finished after 1035 timesteps\n",
      "Episode finished after 1058 timesteps\n",
      "Episode finished after 1079 timesteps\n",
      "Episode finished after 1115 timesteps\n",
      "Episode finished after 1139 timesteps\n",
      "Episode finished after 1160 timesteps\n",
      "Episode finished after 1174 timesteps\n",
      "Episode finished after 1190 timesteps\n",
      "Episode finished after 1211 timesteps\n",
      "Episode finished after 1240 timesteps\n",
      "Episode finished after 1250 timesteps\n",
      "Episode finished after 1268 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:17:03,375] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('agent', 13)\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 50 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 102 timesteps\n",
      "Episode finished after 116 timesteps\n",
      "Episode finished after 159 timesteps\n",
      "Episode finished after 173 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 224 timesteps\n",
      "Episode finished after 240 timesteps\n",
      "Episode finished after 261 timesteps\n",
      "Episode finished after 286 timesteps\n",
      "Episode finished after 311 timesteps\n",
      "Episode finished after 331 timesteps\n",
      "Episode finished after 347 timesteps\n",
      "Episode finished after 368 timesteps\n",
      "Episode finished after 385 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 435 timesteps\n",
      "Episode finished after 460 timesteps\n",
      "Episode finished after 476 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 546 timesteps\n",
      "Episode finished after 563 timesteps\n",
      "Episode finished after 581 timesteps\n",
      "Episode finished after 605 timesteps\n",
      "Episode finished after 622 timesteps\n",
      "Episode finished after 663 timesteps\n",
      "Episode finished after 710 timesteps\n",
      "Episode finished after 744 timesteps\n",
      "Episode finished after 759 timesteps\n",
      "Episode finished after 771 timesteps\n",
      "Episode finished after 796 timesteps\n",
      "Episode finished after 809 timesteps\n",
      "Episode finished after 845 timesteps\n",
      "Episode finished after 874 timesteps\n",
      "Episode finished after 912 timesteps\n",
      "Episode finished after 934 timesteps\n",
      "Episode finished after 946 timesteps\n",
      "Episode finished after 973 timesteps\n",
      "Episode finished after 989 timesteps\n",
      "Episode finished after 1017 timesteps\n",
      "Episode finished after 1035 timesteps\n",
      "Episode finished after 1067 timesteps\n",
      "Episode finished after 1077 timesteps\n",
      "Episode finished after 1102 timesteps\n",
      "Episode finished after 1117 timesteps\n",
      "Episode finished after 1164 timesteps\n",
      "Episode finished after 1184 timesteps\n",
      "Episode finished after 1263 timesteps\n",
      "Episode finished after 1284 timesteps\n",
      "Episode finished after 1319 timesteps\n",
      "Episode finished after 1349 timesteps\n",
      "Episode finished after 1364 timesteps\n",
      "Episode finished after 1418 timesteps\n",
      "Episode finished after 1436 timesteps\n",
      "Episode finished after 1448 timesteps\n",
      "Episode finished after 1464 timesteps\n",
      "Episode finished after 1501 timesteps\n",
      "Episode finished after 1535 timesteps\n",
      "Episode finished after 1555 timesteps\n",
      "Episode finished after 1584 timesteps\n",
      "Episode finished after 1607 timesteps\n",
      "Episode finished after 1631 timesteps\n",
      "Episode finished after 1647 timesteps\n",
      "Episode finished after 1657 timesteps\n",
      "Episode finished after 1688 timesteps\n",
      "Episode finished after 1696 timesteps\n",
      "Episode finished after 1719 timesteps\n",
      "Episode finished after 1731 timesteps\n",
      "Episode finished after 1746 timesteps\n",
      "Episode finished after 1773 timesteps\n",
      "Episode finished after 1834 timesteps\n",
      "Episode finished after 1846 timesteps\n",
      "Episode finished after 1879 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:17:35,091] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1898 timesteps\n",
      "('agent', 14)\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 44 timesteps\n",
      "Episode finished after 64 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 22:17:36,724] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 88 timesteps\n",
      "Episode finished after 104 timesteps\n",
      "Episode finished after 115 timesteps\n",
      "Episode finished after 127 timesteps\n",
      "Episode finished after 147 timesteps\n",
      "Episode finished after 180 timesteps\n",
      "Episode finished after 232 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 297 timesteps\n",
      "Episode finished after 314 timesteps\n",
      "Episode finished after 325 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 369 timesteps\n",
      "Episode finished after 386 timesteps\n",
      "Episode finished after 405 timesteps\n",
      "Episode finished after 436 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 463 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 491 timesteps\n",
      "Episode finished after 517 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "('agent', 15)\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 51 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 113 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 176 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 205 timesteps\n",
      "Episode finished after 227 timesteps\n",
      "Episode finished after 244 timesteps\n",
      "Episode finished after 275 timesteps\n",
      "Episode finished after 298 timesteps\n",
      "Episode finished after 354 timesteps\n",
      "Episode finished after 370 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 412 timesteps\n",
      "Episode finished after 439 timesteps\n",
      "Episode finished after 458 timesteps\n",
      "Episode finished after 483 timesteps\n",
      "Episode finished after 495 timesteps\n",
      "Episode finished after 534 timesteps\n",
      "Episode finished after 562 timesteps\n",
      "Episode finished after 575 timesteps\n",
      "Episode finished after 601 timesteps\n",
      "Episode finished after 625 timesteps\n",
      "Episode finished after 644 timesteps\n",
      "Episode finished after 660 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    print('agent', i)\n",
    "    envs[i] = gym.make(game_type)\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    length_game = random.randint(nwindow, init_moves)\n",
    "    #print(state[:,i].shape)\n",
    "    #print(envs[i].reset().shape)\n",
    "    observation_prev = np.zeros((4)) # initiailzing frame t-2\n",
    "    observation = envs[i].reset()\n",
    "    for t in range(length_game):\n",
    "        envs[i].render()\n",
    "        actions[i] = envs[i].action_space.sample()\n",
    "        #print(actions[i])\n",
    "        observation_prev = observation\n",
    "        observation, rewards[i], dones[i], _ = envs[i].step(actions[i])\n",
    "        #print(rewards[i])\n",
    "        block_reward[i] += rewards[i]\n",
    "        total_epochs[i] += 1\n",
    "        if dones[i]:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            block_reward[i] = 0.0 # resettig total reward\n",
    "            envs[i].reset()\n",
    "    rewards[i] = block_reward[i]\n",
    "    state[:, i] = np.hstack([observation, float(1), observation_prev, float(1)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Block_Reward = np.sum(block_reward)\n",
    "Block_Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01409005,  0.16856855,  0.0179468 , -0.31191618,  1.        ,\n",
       "        0.01461573, -0.02628404,  0.01844902, -0.02511075,  1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[:, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_reward = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinli/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:66: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/kevinli/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:60: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n",
      "/Users/kevinli/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:67: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/kevinli/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:69: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (16,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6d007ceb5a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0my_actual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_actual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmsprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a6e365bb09a6>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, s, a, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mdh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,) (16,) (200,) "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "last_epochs=0\n",
    "#epsilon = 0.2\n",
    "\n",
    "y_actual = np.zeros((npar), dtype=float)\n",
    "a_actual = np.zeros((npar), dtype=float)\n",
    "print(a_actual)\n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "  \n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################\n",
    "    indx = int(istep/(nsteps/(1.0*neps)))\n",
    "    epsilon = epsilons[indx]\n",
    "    lr = learning_rates[indx]\n",
    "    update_estimator(q_estimator, target_estimator, nwindow, istep)\n",
    "    rew, _ = q_estimator.predict(state)\n",
    "    A = policy(q_estimator, state, epsilon)\n",
    "    for j in range(npar):\n",
    "        # ------- Choosing the action using the Greedy Policy ---------- #\n",
    "        action_prob = A[:, j]\n",
    "        sampling_prob = np.random.uniform()\n",
    "        action = int(sampling_prob > action_prob[0])\n",
    "        #print(action)\n",
    "        #print(a_actual)\n",
    "        a_actual[j] = action\n",
    "        state[5:9, j] = state[0:4, j]\n",
    "        state[0:4, j], rewards[j], dones[j], _ = envs[j].step(action)\n",
    "        if dones[j]:\n",
    "            rewards[j] = 0\n",
    "        y_actual[j] = rewards[j]\n",
    "    q_estimator.gradient(state, a_actual, y_actual) \n",
    "    q_estimator.rmsprop()    \n",
    "        \n",
    "        \n",
    "    Block_Reward += np.sum(rewards)\n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0):     \n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, Block_Reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        Block_Reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23530939,  0.55745888,  0.92382342,  0.39634419,  0.28210271,\n",
       "        0.92657936,  0.41145885,  0.75122915,  0.86642856,  0.56349944])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "state[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21600844,  0.25735466,  0.86689874,  0.91900546])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[5:9, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
